{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\",\n",
    "    \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\", \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\",\n",
    "    \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\", \"ps_calc_18_bin\",\n",
    "    \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "dfTrain = pd.read_csv(train_file)\n",
    "dfTest = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([dfTrain, dfTest])\n",
    "# 特征字典，key是每一列，即每个field，value是每个值对应的feature_id\n",
    "feature_dict = {}\n",
    "# 特征总数量\n",
    "total_feature = 0\n",
    "for col in df.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        # 数字类型列，作为一个特征\n",
    "        feature_dict[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        # 查看这一列有多少个unique的值\n",
    "        unique_val = df[col].unique()\n",
    "        feature_dict[col] = dict(\n",
    "            zip(unique_val,\n",
    "                range(total_feature,\n",
    "                      len(unique_val) + total_feature)))\n",
    "        total_feature += len(unique_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = dfTrain[['target']].values.tolist()\n",
    "dfTrain.drop(['target', 'id'], axis=1, inplace=True)\n",
    "train_feature_index = dfTrain.copy()\n",
    "train_feature_value = dfTrain.copy()\n",
    "for col in train_feature_index.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        train_feature_index.drop(col, axis=1, inplace=True)\n",
    "        train_feature_value.drop(col, axis=1, inplace=True)\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        train_feature_index[col] = feature_dict[col]\n",
    "    else:\n",
    "        train_feature_index[col] = train_feature_index[col].map(\n",
    "            feature_dict[col])\n",
    "        train_feature_value[col] = 1\n",
    "train_y = np.reshape(np.array(train_y), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "afm_params = {\n",
    "    \"embedding_size\": 8,\n",
    "    \"attention_size\": 10,\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layer_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"verbose\": True,\n",
    "    \"random_seed\": 0,\n",
    "    \"deep_init_size\": 50,\n",
    "    \"use_inner\": False\n",
    "}\n",
    "afm_params['feature_size'] = total_feature\n",
    "afm_params['field_size'] = len(train_feature_index.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict()\n",
    "weights['feature_embeddings'] = tf.Variable(tf.random_normal(\n",
    "    [afm_params['feature_size'], afm_params['embedding_size']],\n",
    "    mean=0.0,\n",
    "    stddev=0.01),\n",
    "                                            name='feauter_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(\n",
    "    [afm_params['feature_size'], 1], mean=0.0, stddev=0.01),\n",
    "                                      name='feature_bias')\n",
    "weights['bias'] = tf.Variable(tf.constant(0.1), name='bias')\n",
    "\n",
    "glorot = 2.0 / (afm_params['attention_size'] + afm_params['embedding_size'])\n",
    "weights['attention_w'] = tf.Variable(tf.random_normal(\n",
    "    [afm_params['embedding_size'], afm_params['attention_size']],\n",
    "    mean=0.0,\n",
    "    stddev=glorot),\n",
    "                                     name='attention_w')\n",
    "weights['attention_b'] = tf.Variable(tf.random_normal([\n",
    "    afm_params['attention_size'],\n",
    "],\n",
    "                                                      mean=0.0,\n",
    "                                                      stddev=glorot),\n",
    "                                     name='attention_b')\n",
    "weights['attention_h'] = tf.Variable(tf.random_normal([\n",
    "    afm_params['attention_size'],\n",
    "],\n",
    "                                                      mean=0.0,\n",
    "                                                      stddev=glorot),\n",
    "                                     name='attention_b')\n",
    "weights['attention_p'] = tf.Variable(tf.random_normal(\n",
    "    [afm_params['embedding_size'], 1], mean=0.0, stddev=glorot),\n",
    "                                     name='attention_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_index = tf.placeholder(tf.int32, shape=[None, None], name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32, shape=[None, None], name='feat_value')\n",
    "label = tf.placeholder(tf.float32, shape=[None, 1], name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.nn.embedding_lookup(weights['feature_embeddings'], feat_index)\n",
    "reshaped_feat_value = tf.reshape(feat_value, [-1, afm_params['field_size'], 1])\n",
    "embeddings = tf.multiply(embeddings, reshaped_feat_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first order part\n",
    "y_first_order = tf.nn.embedding_lookup(weights['feature_bias'], feat_index)\n",
    "y_first_order_output = tf.reduce_sum(tf.multiply(y_first_order, reshaped_feat_value),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-32e58f7e7fdb>:30: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-10-32e58f7e7fdb>:34: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# element wise-> vi * vj\n",
    "element_wise_product_list = []\n",
    "for i in range(0, afm_params['field_size'] - 1):\n",
    "    for j in range(i + 1, afm_params['field_size']):\n",
    "        element_wise_product_list.append(\n",
    "            tf.multiply(embeddings[:, i, :], embeddings[:, j, :]))\n",
    "# (666, ?, 8)\n",
    "element_wise_product = tf.stack(element_wise_product_list)\n",
    "# (?, 666, 8)\n",
    "element_wise_product = tf.transpose(element_wise_product,\n",
    "                                    perm=[1, 0, 2],\n",
    "                                    name='element_wise_product')\n",
    "\n",
    "# attention part\n",
    "num_iteractions = int(afm_params['field_size'] *\n",
    "                      (afm_params['field_size'] - 1) / 2)\n",
    "# (?, 666, 10)\n",
    "attention_wx_plus_b = tf.reshape(\n",
    "    tf.matmul(\n",
    "        tf.reshape(element_wise_product_list,\n",
    "                   shape=(-1, afm_params['embedding_size'])),\n",
    "        weights['attention_w']) + weights['attention_b'],\n",
    "    [-1, num_iteractions, afm_params['attention_size']])\n",
    "\n",
    "# (?, 666, 1)\n",
    "attention_exp = tf.exp(\n",
    "    tf.reduce_sum(tf.multiply(tf.nn.relu(attention_wx_plus_b),\n",
    "                              weights['attention_h']),\n",
    "                  axis=2,\n",
    "                  keep_dims=True))\n",
    "# (?, 1, 1)\n",
    "attention_exp_sum = tf.reduce_sum(attention_exp, axis=1, keep_dims=True)\n",
    "# (?, 666, 1)\n",
    "attention_out = tf.div(attention_exp, attention_exp_sum, name='attention_out')\n",
    "# (?, 8)\n",
    "attention_x_product = tf.reduce_sum(tf.multiply(attention_out,\n",
    "                                                element_wise_product),\n",
    "                                    axis=1,\n",
    "                                    name='afm')\n",
    "attention_part_sum = tf.matmul(attention_x_product, weights['attention_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "y_bias = weights['bias'] * tf.ones_like(label)\n",
    "out = tf.add_n([tf.reduce_sum(y_first_order_output, axis=1, keep_dims=True),\n",
    "               attention_part_sum, y_bias])\n",
    "out = tf.nn.sigmoid(out)\n",
    "loss = tf.losses.log_loss(label, out)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=afm_params['learning_rate'],\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999,\n",
    "                                   epsilon=1e-8).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7524369\n",
      "0.73426753\n",
      "0.7164515\n",
      "0.69899505\n",
      "0.68190384\n",
      "0.665183\n",
      "0.6488371\n",
      "0.63286984\n",
      "0.61728424\n",
      "0.60315657\n",
      "0.5885144\n",
      "0.57426316\n",
      "0.560402\n",
      "0.5469296\n",
      "0.5338442\n",
      "0.5211432\n",
      "0.50882363\n",
      "0.4968817\n",
      "0.48174083\n",
      "0.4702792\n",
      "0.45916504\n",
      "0.4483965\n",
      "0.4379704\n",
      "0.4278827\n",
      "0.41812855\n",
      "0.40870252\n",
      "0.39959845\n",
      "0.39354745\n",
      "0.38513884\n",
      "0.37702805\n",
      "0.36920774\n",
      "0.3616704\n",
      "0.3544082\n",
      "0.34741324\n",
      "0.34067762\n",
      "0.33419308\n",
      "0.32002747\n",
      "0.31384528\n",
      "0.30787823\n",
      "0.30212092\n",
      "0.29656765\n",
      "0.29121262\n",
      "0.2860498\n",
      "0.28107333\n",
      "0.27627707\n",
      "0.28264448\n",
      "0.27843407\n",
      "0.27439252\n",
      "0.27051222\n",
      "0.26678607\n",
      "0.2632072\n",
      "0.259769\n",
      "0.2564652\n",
      "0.25328982\n",
      "0.25130227\n",
      "0.24841407\n",
      "0.24563567\n",
      "0.24296224\n",
      "0.24038917\n",
      "0.23791203\n",
      "0.23552662\n",
      "0.2332289\n",
      "0.23101512\n",
      "0.2354325\n",
      "0.23346788\n",
      "0.23157686\n",
      "0.22975603\n",
      "0.22800203\n",
      "0.2263118\n",
      "0.2246824\n",
      "0.22311103\n",
      "0.22159505\n",
      "0.2297241\n",
      "0.22841144\n",
      "0.22714776\n",
      "0.2259306\n",
      "0.22475767\n",
      "0.22362678\n",
      "0.2225359\n",
      "0.22148313\n",
      "0.22046667\n",
      "0.202573\n",
      "0.20153496\n",
      "0.20051938\n",
      "0.1995263\n",
      "0.19855571\n",
      "0.1976075\n",
      "0.19668147\n",
      "0.19577745\n",
      "0.19489516\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size = int(len(train_feature_index)/afm_params['batch_size'])\n",
    "    for i in range(afm_params['epoch']):\n",
    "        for j in range(batch_size):\n",
    "            start = i * afm_params['batch_size']\n",
    "            end = (i+1) * afm_params['batch_size']\n",
    "            end = end if end<len(train_feature_index) else len(train_feature_index)\n",
    "            feat_index_batch = train_feature_index[start:end]\n",
    "            feat_value_batch = train_feature_value[start:end]\n",
    "            label_batch = train_y[start:end]\n",
    "            feed_dict = {\n",
    "                feat_index:feat_index_batch,\n",
    "                feat_value:feat_value_batch,\n",
    "                label:label_batch\n",
    "            }\n",
    "            l,o = sess.run([loss,optimizer], feed_dict)\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
