{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\",\n",
    "    \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\", \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\",\n",
    "    \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\", \"ps_calc_18_bin\",\n",
    "    \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "dfTrain = pd.read_csv(train_file)\n",
    "dfTest = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([dfTrain, dfTest])\n",
    "# 特征字典，key是每一列，即每个field，value是每个值对应的feature_id\n",
    "feature_dict = {}\n",
    "# 特征总数量\n",
    "total_feature = 0\n",
    "for col in df.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        # 数字类型列，作为一个特征\n",
    "        feature_dict[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        # 查看这一列有多少个unique的值\n",
    "        unique_val = df[col].unique()\n",
    "        feature_dict[col] = dict(\n",
    "            zip(unique_val,\n",
    "                range(total_feature,\n",
    "                      len(unique_val) + total_feature)))\n",
    "        total_feature += len(unique_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = dfTrain[['target']].values.tolist()\n",
    "dfTrain.drop(['target', 'id'], axis=1, inplace=True)\n",
    "train_feature_index = dfTrain.copy()\n",
    "train_feature_value = dfTrain.copy()\n",
    "for col in train_feature_index.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        train_feature_index.drop(col, axis=1, inplace=True)\n",
    "        train_feature_value.drop(col, axis=1, inplace=True)\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        train_feature_index[col] = feature_dict[col]\n",
    "    else:\n",
    "        train_feature_index[col] = train_feature_index[col].map(\n",
    "            feature_dict[col])\n",
    "        train_feature_value[col] = 1\n",
    "train_y = np.reshape(np.array(train_y), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm_params = {\n",
    "    \"embedding_size\": 8,\n",
    "    \"deep_layer_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"verbose\": True,\n",
    "    \"random_seed\": 0,\n",
    "    \"deep_init_size\": 50,\n",
    "    \"use_inner\": False\n",
    "}\n",
    "ffm_params['feature_size'] = total_feature\n",
    "ffm_params['field_size'] = len(train_feature_index.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict()\n",
    "\n",
    "feature_embeddings = tf.Variable(tf.random_normal([\n",
    "    ffm_params['field_size'], ffm_params['feature_size'],\n",
    "    ffm_params['embedding_size']\n",
    "],\n",
    "                                                  mean=0.0,\n",
    "                                                  stddev=0.01),\n",
    "                                 name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(\n",
    "    [ffm_params['feature_size'], 1], mean=0.0, stddev=0.01),\n",
    "                                      name='feature_bias')\n",
    "weights['bias'] = tf.Variable(tf.constant(0.1), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_index = tf.placeholder(tf.int32, shape=[None, ffm_params['field_size']], name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32, shape=[None, ffm_params['field_size']], name='feat_value')\n",
    "label = tf.placeholder(tf.float32, shape=[None, 1], name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# linear part\n",
    "linear_weight = tf.nn.embedding_lookup(weights['feature_bias'], feat_index)\n",
    "reshaped_feat_val = tf.reshape(feat_value, [-1, ffm_params['field_size'], 1])\n",
    "linear_output = tf.reduce_sum( tf.multiply(linear_weight, reshaped_feat_val), axis=1)\n",
    "print(linear_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_res = tf.ones_like(label)\n",
    "for i in range(ffm_params['field_size']):\n",
    "    for j in range(i + 1, ffm_params['field_size']):\n",
    "        # 第i个特征与第j个field的隐向量\n",
    "        v_dj_fi = tf.nn.embedding_lookup(feature_embeddings[j],\n",
    "                                         feat_index[:, i])\n",
    "        # 第j个特征与第i个field的隐向量\n",
    "        v_di_fj = tf.nn.embedding_lookup(feature_embeddings[i],\n",
    "                                         feat_index[:, j])\n",
    "        second_order_res += tf.reduce_sum(tf.multiply(v_dj_fi, v_di_fj), 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "output = linear_output + second_order_res\n",
    "output = tf.nn.sigmoid(second_order_res)\n",
    "loss = tf.losses.log_loss(label, output)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=ffm_params['learning_rate'], \n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999,\n",
    "                                   epsilon=1e-8).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2712694\n",
      "1.225049\n",
      "1.1792042\n",
      "1.1328077\n",
      "1.0851325\n",
      "1.0356503\n",
      "0.9840219\n",
      "0.93009543\n",
      "0.87391263\n",
      "0.81571716\n",
      "0.75659555\n",
      "0.69652325\n",
      "0.6362619\n",
      "0.57670915\n",
      "0.5188507\n",
      "0.46370173\n",
      "0.41223383\n",
      "0.36529678\n",
      "0.31694987\n",
      "0.27926198\n",
      "0.24718258\n",
      "0.22062233\n",
      "0.19925454\n",
      "0.18257576\n",
      "0.16997573\n",
      "0.16080464\n",
      "0.15442625\n",
      "0.15671447\n",
      "0.15477328\n",
      "0.15405127\n",
      "0.15418889\n",
      "0.15489872\n",
      "0.15595596\n",
      "0.15718845\n",
      "0.15846685\n",
      "0.15969652\n",
      "0.13473457\n",
      "0.13552928\n",
      "0.13620517\n",
      "0.13674656\n",
      "0.1371445\n",
      "0.13739613\n",
      "0.1375027\n",
      "0.13746907\n",
      "0.13730244\n",
      "0.17302518\n",
      "0.17281619\n",
      "0.17236349\n",
      "0.17169702\n",
      "0.17084585\n",
      "0.16983832\n",
      "0.16870189\n",
      "0.16746297\n",
      "0.16614649\n",
      "0.17011681\n",
      "0.16893521\n",
      "0.16768533\n",
      "0.16639495\n",
      "0.16508822\n",
      "0.16378605\n",
      "0.16250628\n",
      "0.16126372\n",
      "0.1600703\n",
      "0.17583936\n",
      "0.17485996\n",
      "0.17387125\n",
      "0.17289378\n",
      "0.17194319\n",
      "0.17103052\n",
      "0.17016289\n",
      "0.16934378\n",
      "0.16857363\n",
      "0.18555841\n",
      "0.18509223\n",
      "0.18462701\n",
      "0.1841674\n",
      "0.18371469\n",
      "0.18326777\n",
      "0.18282387\n",
      "0.182379\n",
      "0.18192863\n",
      "0.16529718\n",
      "0.16543846\n",
      "0.16545688\n",
      "0.16535772\n",
      "0.16514878\n",
      "0.16483964\n",
      "0.1644414\n",
      "0.16396575\n",
      "0.1634247\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size = int(len(train_feature_index)/ffm_params['batch_size'])\n",
    "    for i in range(ffm_params['epoch']):\n",
    "        for j in range(batch_size):\n",
    "            start = i * ffm_params['batch_size']\n",
    "            end = (i+1) * ffm_params['batch_size']\n",
    "            end = end if end<len(train_feature_index) else len(train_feature_index)\n",
    "            feat_index_batch = train_feature_index[start:end]\n",
    "            feat_value_batch = train_feature_value[start:end]\n",
    "            label_batch = train_y[start:end]\n",
    "            feed_dict = {\n",
    "                feat_index:feat_index_batch,\n",
    "                feat_value:feat_value_batch,\n",
    "                label:label_batch\n",
    "            }\n",
    "            l,o = sess.run([loss,optimizer], feed_dict)\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
