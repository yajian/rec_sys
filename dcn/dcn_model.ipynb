{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\",\n",
    "    \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\", \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\",\n",
    "    \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\", \"ps_calc_18_bin\",\n",
    "    \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "dfTrain = pd.read_csv(train_file)\n",
    "dfTest = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([dfTrain, dfTest])\n",
    "# 特征字典，key是每一列，即每个field，value是每个值对应的feature_id\n",
    "feature_dict = {}\n",
    "# 特征总数量\n",
    "total_feature = 0\n",
    "for col in df.columns:\n",
    "    if col in IGNORE_COLS or col in NUMERIC_COLS:\n",
    "        continue\n",
    "    else:\n",
    "        # 查看这一列有多少个unique的值\n",
    "        unique_val = df[col].unique()\n",
    "        feature_dict[col] = dict(\n",
    "            zip(unique_val,\n",
    "                range(total_feature,\n",
    "                      len(unique_val) + total_feature)))\n",
    "        total_feature += len(unique_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = dfTrain[['target']].values.tolist()\n",
    "dfTrain.drop(['target', 'id'], axis=1, inplace=True)\n",
    "# 获取类型为numeric列的值\n",
    "train_feature_numeric_value = dfTrain[NUMERIC_COLS]\n",
    "dfTrain.drop(NUMERIC_COLS,axis=1,inplace=True)\n",
    "\n",
    "train_feature_index = dfTrain.copy()\n",
    "train_feature_value = dfTrain.copy()\n",
    "for col in train_feature_index.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        train_feature_index.drop(col, axis=1, inplace=True)\n",
    "        train_feature_value.drop(col, axis=1, inplace=True)\n",
    "        continue\n",
    "    else:\n",
    "        train_feature_index[col] = train_feature_index[col].map(feature_dict[col])\n",
    "        train_feature_value[col] = 1\n",
    "train_y = np.reshape(np.array(train_y), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "30\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "dcn_params = {\n",
    "    \"embedding_size\": 8,\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"random_seed\": 0,\n",
    "    \"cross_layer_num\":3,\n",
    "    \"numeric_feature_size\": len(NUMERIC_COLS)\n",
    "}\n",
    "dcn_params['feature_size'] = total_feature\n",
    "dcn_params['field_size'] = len(train_feature_index.columns)\n",
    "print(total_feature)\n",
    "print(dcn_params['field_size'])\n",
    "print(len(NUMERIC_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict()\n",
    "weights['feature_embeddings'] = tf.Variable(tf.random_normal(\n",
    "    [dcn_params['feature_size'], dcn_params['embedding_size']],\n",
    "    mean=0.0,\n",
    "    stddev=0.01),\n",
    "                                            name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(\n",
    "    [dcn_params['feature_size'], 1], mean=0.0, stddev=0.01),\n",
    "                                      name='feature_bias')\n",
    "total_size = dcn_params['field_size'] * dcn_params[\n",
    "    'embedding_size'] + dcn_params['numeric_feature_size']\n",
    "num_layer = len(dcn_params['deep_layers'])\n",
    "glorot = np.sqrt(2.0 / (total_size + dcn_params['deep_layers'][0]))\n",
    "\n",
    "weights['layer_0'] = tf.Variable(\n",
    "    tf.random_normal([total_size, dcn_params['deep_layers'][0]],\n",
    "                     mean=0.0,\n",
    "                     stddev=glorot))\n",
    "weights['bias_0'] = tf.Variable(\n",
    "    tf.random_normal([1, dcn_params['deep_layers'][0]],\n",
    "                     mean=0.0,\n",
    "                     stddev=glorot))\n",
    "for i in range(1, num_layer):\n",
    "    glorot = np.sqrt(\n",
    "        2.0 /\n",
    "        (dcn_params['deep_layers'][i - 1] + dcn_params['deep_layers'][i]))\n",
    "    weights['layer_%d' % i] = tf.Variable(\n",
    "        tf.random_normal(\n",
    "            [dcn_params['deep_layers'][i - 1], dcn_params['deep_layers'][i]],\n",
    "            mean=0.0,\n",
    "            stddev=glorot))\n",
    "    weights['bias_%d' % i] = tf.Variable(\n",
    "        tf.random_normal([1, dcn_params['deep_layers'][i]],\n",
    "                         mean=0.0,\n",
    "                         stddev=glorot))\n",
    "for i in range(dcn_params['cross_layer_num']):\n",
    "    weights[\"cross_layer_%d\" % i] = tf.Variable(\n",
    "        tf.random_normal([total_size, 1], mean=0.0, stddev=glorot))\n",
    "    weights[\"cross_bias_%d\" % i] = tf.Variable(\n",
    "        tf.random_normal([total_size, 1], mean=0.0, stddev=glorot))\n",
    "\n",
    "input_size = total_size + dcn_params['deep_layers'][-1]\n",
    "\n",
    "glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "weights['concat_projection'] = tf.Variable(\n",
    "    tf.random_normal([input_size, 1], mean=0.0, stddev=glorot))\n",
    "weights['concat_bias'] = tf.Variable(tf.constant(0.01), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_index = tf.placeholder(tf.int32, shape=[None, dcn_params['field_size']], name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32, shape=[None, dcn_params['field_size']], name='feat_value')\n",
    "numeric_value = tf.placeholder(tf.float32,\n",
    "                               shape=[None, dcn_params['numeric_feature_size']],\n",
    "                               name='num_value')\n",
    "label = tf.placeholder(tf.float32, shape=[None, 1], name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.nn.embedding_lookup(weights['feature_embeddings'], feat_index)\n",
    "reshaped_feat_value = tf.reshape(feat_value,\n",
    "                                 shape=[-1, dcn_params['field_size'], 1])\n",
    "embeddings = tf.multiply(embeddings, reshaped_feat_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 247)\n",
      "(?, 32)\n",
      "(?, 247)\n"
     ]
    }
   ],
   "source": [
    "# numeric_size + field_size * embedding_size\n",
    "x0 = tf.concat([\n",
    "    numeric_value,\n",
    "    tf.reshape(embeddings,\n",
    "               [-1, dcn_params['field_size'] * dcn_params['embedding_size']])\n",
    "],\n",
    "               axis=1)\n",
    "# deep part\n",
    "y_deep = x0\n",
    "for i in range(0, len(dcn_params['deep_layers'])):\n",
    "    y_deep = tf.matmul(y_deep,\n",
    "                       weights['layer_%d' % i]) + weights['bias_%d' % i]\n",
    "    y_deep = dcn_params['deep_layers_activation'](y_deep)\n",
    "\n",
    "# y_deep: (?, dcn_params['deep_layers'][-1])\n",
    "\n",
    "# cross part\n",
    "\n",
    "x0_reshape = tf.reshape(x0, [-1, total_size, 1])\n",
    "\n",
    "# 这种实现方式占内存\n",
    "# for l in range(dcn_params['cross_layer_num']):\n",
    "#     # 这里存疑\n",
    "#     x_l = tf.tensordot(tf.matmul(x0_reshape, x_l, transpose_b=True),\n",
    "#                        weights[\"cross_layer_%d\" % l],\n",
    "#                        axes=1) + weights['cross_bias_%d' % i] + x_l\n",
    "x_l = x0_reshape\n",
    "for l in range(dcn_params['cross_layer_num']):\n",
    "    xb = tf.tensordot(tf.reshape(x_l, [-1, 1, total_size]),\n",
    "                      weights[\"cross_layer_%d\" % l], 1)\n",
    "    x_l = tf.multiply(x0_reshape, xb) + weights['cross_bias_%d' % i] + x_l\n",
    "cross_network_out = tf.reshape(x_l, (-1, total_size))\n",
    "\n",
    "# concat part\n",
    "concat_input = tf.concat([cross_network_out, y_deep], axis=1)\n",
    "print(cross_network_out.shape)\n",
    "print(y_deep.shape)\n",
    "print(x0.shape)\n",
    "out = tf.matmul(concat_input,\n",
    "                weights['concat_projection']) + weights['concat_bias']\n",
    "out = tf.nn.sigmoid(out)\n",
    "loss = tf.losses.log_loss(label, out)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=dcn_params['learning_rate'],\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999,\n",
    "                                   epsilon=1e-8).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1480167\n",
      "1.0077935\n",
      "0.88329786\n",
      "0.77332556\n",
      "0.676273\n",
      "0.59070575\n",
      "0.5155548\n",
      "0.45002508\n",
      "0.39344358\n",
      "0.3488221\n",
      "0.3094035\n",
      "0.2769541\n",
      "0.25069693\n",
      "0.22984628\n",
      "0.21364105\n",
      "0.20135972\n",
      "0.19233568\n",
      "0.18596679\n",
      "0.16188584\n",
      "0.15811683\n",
      "0.15567833\n",
      "0.1542659\n",
      "0.15362176\n",
      "0.15353443\n",
      "0.15382954\n",
      "0.15436718\n",
      "0.15503475\n",
      "0.16017298\n",
      "0.16103034\n",
      "0.1617975\n",
      "0.16244242\n",
      "0.16294551\n",
      "0.16329738\n",
      "0.16349743\n",
      "0.16355018\n",
      "0.16346508\n",
      "0.14118245\n",
      "0.14098166\n",
      "0.14070672\n",
      "0.14037105\n",
      "0.13998735\n",
      "0.1395668\n",
      "0.13912013\n",
      "0.13865685\n",
      "0.13818575\n",
      "0.1713267\n",
      "0.17063288\n",
      "0.16988662\n",
      "0.16910955\n",
      "0.16832158\n",
      "0.16753843\n",
      "0.16677485\n",
      "0.1660431\n",
      "0.16535157\n",
      "0.16931258\n",
      "0.16873437\n",
      "0.16819037\n",
      "0.16768612\n",
      "0.16722275\n",
      "0.16680124\n",
      "0.16642162\n",
      "0.16608247\n",
      "0.16577917\n",
      "0.17687869\n",
      "0.1766331\n",
      "0.17639929\n",
      "0.17617798\n",
      "0.17596902\n",
      "0.1757722\n",
      "0.1755852\n",
      "0.17540625\n",
      "0.17523368\n",
      "0.1909211\n",
      "0.1907798\n",
      "0.19063514\n",
      "0.19048896\n",
      "0.19034213\n",
      "0.19019483\n",
      "0.19004712\n",
      "0.18990093\n",
      "0.18975554\n",
      "0.16511305\n",
      "0.16510352\n",
      "0.16505602\n",
      "0.16497931\n",
      "0.16487654\n",
      "0.164748\n",
      "0.16459806\n",
      "0.16443194\n",
      "0.16425292\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size = int(len(train_feature_index)/dcn_params['batch_size'])\n",
    "    for i in range(dcn_params['epoch']):\n",
    "        for j in range(batch_size):\n",
    "            start = i * dcn_params['batch_size']\n",
    "            end = (i+1) * dcn_params['batch_size']\n",
    "            end = end if end<len(train_feature_index) else len(train_feature_index)\n",
    "            feat_index_batch = train_feature_index[start:end]\n",
    "            feat_value_batch = train_feature_value[start:end]\n",
    "            feat_numeric_value_batch = train_feature_numeric_value[start:end]\n",
    "            label_batch = train_y[start:end]\n",
    "            feed_dict = {\n",
    "                feat_index:feat_index_batch,\n",
    "                feat_value:feat_value_batch,\n",
    "                numeric_value:feat_numeric_value_batch,\n",
    "                label:label_batch\n",
    "            }\n",
    "            l,o = sess.run([loss,optimizer], feed_dict)\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
