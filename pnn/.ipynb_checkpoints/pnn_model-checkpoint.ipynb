{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\",\n",
    "    \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\",\n",
    "    \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\",\n",
    "    \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\",\n",
    "    \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "dfTrain = pd.read_csv('../data/train.csv')\n",
    "dfTest = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ps_car_01_cat': {10: 0, 11: 1, 7: 2, 6: 3, 9: 4, 5: 5, 4: 6, 8: 7, 3: 8, 0: 9, 2: 10, 1: 11, -1: 12}, 'ps_car_02_cat': {1: 13, 0: 14}, 'ps_car_03_cat': {-1: 15, 0: 16, 1: 17}, 'ps_car_04_cat': {0: 18, 1: 19, 8: 20, 9: 21, 2: 22, 6: 23, 3: 24, 7: 25, 4: 26, 5: 27}, 'ps_car_05_cat': {1: 28, -1: 29, 0: 30}, 'ps_car_06_cat': {4: 31, 11: 32, 14: 33, 13: 34, 6: 35, 15: 36, 3: 37, 0: 38, 1: 39, 10: 40, 12: 41, 9: 42, 17: 43, 7: 44, 8: 45, 5: 46, 2: 47, 16: 48}, 'ps_car_07_cat': {1: 49, -1: 50, 0: 51}, 'ps_car_08_cat': {0: 52, 1: 53}, 'ps_car_09_cat': {0: 54, 2: 55, 3: 56, 1: 57, -1: 58, 4: 59}, 'ps_car_10_cat': {1: 60, 0: 61, 2: 62}, 'ps_car_11': {2: 63, 3: 64, 1: 65, 0: 66}, 'ps_car_11_cat': {12: 67, 19: 68, 60: 69, 104: 70, 82: 71, 99: 72, 30: 73, 68: 74, 20: 75, 36: 76, 101: 77, 103: 78, 41: 79, 59: 80, 43: 81, 64: 82, 29: 83, 95: 84, 24: 85, 5: 86, 28: 87, 87: 88, 66: 89, 10: 90, 26: 91, 54: 92, 32: 93, 38: 94, 83: 95, 89: 96, 49: 97, 93: 98, 1: 99, 22: 100, 85: 101, 78: 102, 31: 103, 34: 104, 7: 105, 8: 106, 3: 107, 46: 108, 27: 109, 25: 110, 61: 111, 16: 112, 69: 113, 40: 114, 76: 115, 39: 116, 88: 117, 42: 118, 75: 119, 91: 120, 23: 121, 2: 122, 71: 123, 90: 124, 80: 125, 44: 126, 92: 127, 72: 128, 96: 129, 86: 130, 62: 131, 33: 132, 67: 133, 73: 134, 77: 135, 18: 136, 21: 137, 74: 138, 37: 139, 48: 140, 70: 141, 13: 142, 15: 143, 102: 144, 53: 145, 65: 146, 100: 147, 51: 148, 79: 149, 52: 150, 63: 151, 94: 152, 6: 153, 57: 154, 35: 155, 98: 156, 56: 157, 97: 158, 55: 159, 84: 160, 50: 161, 4: 162, 58: 163, 9: 164, 17: 165, 11: 166, 45: 167, 14: 168, 81: 169, 47: 170}, 'ps_car_12': 171, 'ps_car_13': 172, 'ps_car_14': 173, 'ps_car_15': 174, 'ps_ind_01': {2: 175, 1: 176, 5: 177, 0: 178, 4: 179, 3: 180, 6: 181, 7: 182}, 'ps_ind_02_cat': {2: 183, 1: 184, 4: 185, 3: 186, -1: 187}, 'ps_ind_03': {5: 188, 7: 189, 9: 190, 2: 191, 0: 192, 4: 193, 3: 194, 1: 195, 11: 196, 6: 197, 8: 198, 10: 199}, 'ps_ind_04_cat': {1: 200, 0: 201, -1: 202}, 'ps_ind_05_cat': {0: 203, 1: 204, 4: 205, 3: 206, 6: 207, 5: 208, -1: 209, 2: 210}, 'ps_ind_06_bin': {0: 211, 1: 212}, 'ps_ind_07_bin': {1: 213, 0: 214}, 'ps_ind_08_bin': {0: 215, 1: 216}, 'ps_ind_09_bin': {0: 217, 1: 218}, 'ps_ind_10_bin': {0: 219, 1: 220}, 'ps_ind_11_bin': {0: 221, 1: 222}, 'ps_ind_12_bin': {0: 223, 1: 224}, 'ps_ind_13_bin': {0: 225, 1: 226}, 'ps_ind_14': {0: 227, 1: 228, 2: 229, 3: 230}, 'ps_ind_15': {11: 231, 3: 232, 12: 233, 8: 234, 9: 235, 6: 236, 13: 237, 4: 238, 10: 239, 5: 240, 7: 241, 2: 242, 0: 243, 1: 244}, 'ps_ind_16_bin': {0: 245, 1: 246}, 'ps_ind_17_bin': {1: 247, 0: 248}, 'ps_ind_18_bin': {0: 249, 1: 250}, 'ps_reg_01': 251, 'ps_reg_02': 252, 'ps_reg_03': 253}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([dfTrain, dfTest])\n",
    "# 特征字典，key是每一列，即每个field，value是每个值对应的feature_id\n",
    "feature_dict = {}\n",
    "# 特征总数量\n",
    "total_feature = 0\n",
    "for col in df.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        # 数字类型列，作为一个特征\n",
    "        feature_dict[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        # 查看这一列有多少个unique的值\n",
    "        unique_val = df[col].unique()\n",
    "        feature_dict[col] = dict(\n",
    "            zip(unique_val,\n",
    "                range(total_feature,\n",
    "                      len(unique_val) + total_feature)))\n",
    "        total_feature += len(unique_val)\n",
    "print(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = dfTrain[['target']].values.tolist()\n",
    "dfTrain.drop(['target', 'id'], axis=1, inplace=True)\n",
    "train_feature_index = dfTrain.copy()\n",
    "train_feature_value = dfTrain.copy()\n",
    "for col in train_feature_index.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        train_feature_index.drop(col, axis=1, inplace=True)\n",
    "        train_feature_value.drop(col, axis=1, inplace=True)\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        train_feature_index[col] = feature_dict[col]\n",
    "    else:\n",
    "        train_feature_index[col] = train_feature_index[col].map(\n",
    "            feature_dict[col])\n",
    "        train_feature_value[col] = 1\n",
    "train_y = np.reshape(np.array(train_y), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "\"\"\"模型参数\"\"\"\n",
    "dfm_params = {\n",
    "    \"embedding_size\":8,\n",
    "    \"deep_layers\":[32,32],\n",
    "    \"deep_layer_activation\":tf.nn.relu,\n",
    "    \"epoch\":30,\n",
    "    \"batch_size\":1024,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"optimizer\":\"adam\",\n",
    "    \"batch_norm\":1,\n",
    "    \"batch_norm_decay\":0.995,\n",
    "    \"verbose\":True,\n",
    "    \"random_seed\":0,\n",
    "    \"deep_init_size\":50,\n",
    "    \"use_inner\":False,\n",
    "    \"pairs\": int(len(train_feature_index.columns) * (len(train_feature_index.columns)-1)/2)\n",
    "}\n",
    "dfm_params['feature_size'] = total_feature\n",
    "dfm_params['field_size'] = len(train_feature_index.columns)\n",
    "print(total_feature)\n",
    "print(len(train_feature_index.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_index = tf.placeholder(tf.int32, shape=[None, None], name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32, shape=[None, None], name='feat_value')\n",
    "label = tf.placeholder(tf.float32,shape=[None,1],name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict()\n",
    "\n",
    "weights['feature_embedding'] = tf.Variable(tf.random_normal(\n",
    "    [dfm_params['feature_size'], dfm_params['embedding_size']], 0.0, 0.1),\n",
    "                                           name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(\n",
    "    [dfm_params['feature_size'], 1], 0.0, 0.1),\n",
    "                                      name='feature_bias')\n",
    "if dfm_params['use_inner']:\n",
    "    pass\n",
    "else:\n",
    "    weights['product-quadratic-outer'] = tf.Variable(\n",
    "        tf.random_normal([\n",
    "            dfm_params['embedding_size'], dfm_params['pairs'],\n",
    "            dfm_params['embedding_size']\n",
    "        ], 0.0, 0.1))\n",
    "\n",
    "weights['product-linear'] = tf.Variable(\n",
    "    tf.random_normal([1,dfm_params['field_size']*dfm_params['embedding_size']], \n",
    "                     0.0, 0.01))\n",
    "\n",
    "input_size = dfm_params['embedding_size'] * dfm_params['field_size']+ dfm_params['pairs']\n",
    "glorot = np.sqrt(2.0 / (input_size + dfm_params['deep_layers'][0]))\n",
    "weights['layer_0'] = tf.Variable(np.random.normal(\n",
    "    loc=0, scale=glorot, size=(input_size, dfm_params['deep_layers'][0])),\n",
    "                                 dtype=np.float32)\n",
    "weights['bias_0'] = tf.Variable(np.random.normal(\n",
    "    loc=0, scale=glorot, size=(1, dfm_params['deep_layers'][0])),\n",
    "                                dtype=np.float32)\n",
    "num_layer = len(dfm_params['deep_layers'])\n",
    "for i in range(1, num_layer):\n",
    "    glorot = np.sqrt(\n",
    "        2.0 /\n",
    "        (dfm_params['deep_layers'][i - 1] + dfm_params['deep_layers'][i]))\n",
    "    weights[\"layer_%d\" % i] = tf.Variable(\n",
    "        np.random.normal(loc=0,\n",
    "                         scale=glorot,\n",
    "                         size=(dfm_params['deep_layers'][i - 1],\n",
    "                               dfm_params['deep_layers'][i])),\n",
    "        dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "    weights[\"bias_%d\" % i] = tf.Variable(np.random.normal(\n",
    "        loc=0, scale=glorot, size=(1, dfm_params['deep_layers'][i])),\n",
    "                                         dtype=np.float32)  # 1 * layer[i]\n",
    "\n",
    "glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "weights['output'] = tf.Variable(np.random.normal(\n",
    "    loc=0, scale=glorot, size=(dfm_params['deep_layers'][-1], 1)),\n",
    "                                dtype=np.float32)\n",
    "weights['output_bias'] = tf.Variable(tf.constant(0.01), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "embeddings = tf.nn.embedding_lookup(weights['feature_embedding'], feat_index)\n",
    "reshape_feat_value = tf.reshape(feat_value,\n",
    "                                shape=[-1, dfm_params['field_size'], 1])\n",
    "embeddings = tf.multiply(embeddings, reshape_feat_value)\n",
    "# linear part\n",
    "linear_output = []\n",
    "\n",
    "reshape_embedding = tf.reshape(\n",
    "    embeddings, [-1, dfm_params['field_size'] * dfm_params['embedding_size']])\n",
    "lz = tf.multiply(reshape_embedding, weights['product-linear'])\n",
    "\n",
    "# quadratic part\n",
    "if dfm_params['use_inner']:\n",
    "    row = []\n",
    "    col = []\n",
    "\n",
    "    for i in range(dfm_params['field_size'] - 1):\n",
    "        for j in range(i + 1, dfm_params['field_size']):\n",
    "            row.append(i)\n",
    "            col.append(j)\n",
    "    # tf.transpose(embeddings, [1, 0, 2]) -> (37, ?, 8)\n",
    "    # tf.gather(...) -> (666, ?, 8)\n",
    "    # tf.transpose(...) -> (?, 666, 8)\n",
    "    p = tf.transpose(tf.gather(tf.transpose(embeddings, [1, 0, 2]), row),\n",
    "                     [1, 0, 2])\n",
    "    q = tf.transpose(tf.gather(tf.transpose(embeddings, [1, 0, 2]), col),\n",
    "                     [1, 0, 2])\n",
    "    p = tf.reshape(p, [-1, dfm_params['pairs'], dfm_params['embedding_size']])\n",
    "    q = tf.reshape(q, [-1, dfm_params['pairs'], dfm_params['embedding_size']])\n",
    "    lp = tf.reshape(tf.reduce_sum(p * q, [-1]), [-1, dfm_params['pairs']])\n",
    "else:\n",
    "    row = []\n",
    "    col = []\n",
    "    for i in range(dfm_params['field_size'] - 1):\n",
    "        for j in range(i + 1, dfm_params['field_size']):\n",
    "            row.append(i)\n",
    "            col.append(j)\n",
    "    p = tf.transpose(tf.gather(tf.transpose(embeddings, [1, 0, 2]), row),\n",
    "                     [1, 0, 2])\n",
    "    q = tf.transpose(tf.gather(tf.transpose(embeddings, [1, 0, 2]), col),\n",
    "                     [1, 0, 2])\n",
    "    # (?, 1, 666, 8)->batch * 1 * pair * k\n",
    "    p = tf.expand_dims(p, axis=1)\n",
    "    lp = tf.reduce_sum(\n",
    "        tf.multiply(\n",
    "            tf.transpose(\n",
    "                # (batch, k, pair, k) -> (batch, k, pair)\n",
    "                tf.reduce_sum(\n",
    "                    # (batch * 1 * pair * k) (k * pair * k)=(batch, k, pair, k)\n",
    "                    tf.multiply(p, weights['product-quadratic-outer']),\n",
    "                    axis=-1),\n",
    "                [0, 2, 1]),\n",
    "            q),\n",
    "        axis=-1)\n",
    "    \n",
    "l = tf.concat([lz, lp], axis=1)\n",
    "# deep part\n",
    "\n",
    "for i in range(len(dfm_params['deep_layers'])):\n",
    "    y_deep = tf.add(tf.matmul(l, weights['layer_%d' % i]),weights['bias_%d' % i])\n",
    "    l = dfm_params['deep_layer_activation'](y_deep)\n",
    "\n",
    "out = tf.add(tf.matmul(l, weights['output']), weights['output_bias'])\n",
    "out = tf.nn.sigmoid(out)\n",
    "loss = tf.losses.log_loss(label, out)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=dfm_params['learning_rate'],\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999,\n",
    "                                   epsilon=1e-8).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "0.6971856\n",
      "0.69426274\n",
      "0.6913178\n",
      "0.6882985\n",
      "0.6856513\n",
      "0.6826079\n",
      "0.6800853\n",
      "0.6768952\n",
      "0.6739539\n",
      "0.6709627\n",
      "0.66829187\n",
      "0.66392195\n",
      "0.6615734\n",
      "0.6586554\n",
      "0.6539332\n",
      "0.6517693\n",
      "0.64514637\n",
      "0.6435536\n",
      "0.64030904\n",
      "0.63553286\n",
      "0.6293227\n",
      "0.6305168\n",
      "0.6271801\n",
      "0.6249153\n",
      "0.6195475\n",
      "0.6133213\n",
      "0.6067265\n",
      "0.6022898\n",
      "0.59576684\n",
      "0.5905149\n",
      "0.5855487\n",
      "0.57908195\n",
      "0.5741982\n",
      "0.5571495\n",
      "0.5546503\n",
      "0.53436863\n",
      "0.5317271\n",
      "0.50207675\n",
      "0.49737224\n",
      "0.48402137\n",
      "0.46149322\n",
      "0.44165567\n",
      "0.41190687\n",
      "0.40520346\n",
      "0.37473184\n",
      "0.364684\n",
      "0.32817173\n",
      "0.30407536\n",
      "0.27997422\n",
      "0.26533112\n",
      "0.21113986\n",
      "0.21573842\n",
      "0.20442523\n",
      "0.15578339\n",
      "0.18643521\n",
      "0.11166666\n",
      "0.15621513\n",
      "0.16164088\n",
      "0.12833434\n",
      "0.053876217\n",
      "0.19330655\n",
      "0.19900447\n",
      "0.2534563\n",
      "0.22675635\n",
      "0.18332174\n",
      "0.15662938\n",
      "0.17005171\n",
      "0.16007625\n",
      "0.18012278\n",
      "0.21721712\n",
      "0.23724025\n",
      "0.2770925\n",
      "0.16470027\n",
      "0.2466008\n",
      "0.14415082\n",
      "0.23455313\n",
      "0.10275097\n",
      "0.16665998\n",
      "0.19049218\n",
      "0.16974297\n",
      "0.15896156\n",
      "0.10868257\n",
      "0.18093446\n",
      "0.15474084\n",
      "0.19553483\n",
      "0.16542952\n",
      "0.1560275\n",
      "0.15684395\n",
      "0.17698734\n",
      "0.12003453\n",
      "0.1569738\n",
      "0.16991627\n",
      "0.1210828\n",
      "0.17378911\n",
      "0.09739502\n",
      "0.1512135\n",
      "0.16226393\n",
      "0.12921675\n",
      "0.06454909\n",
      "0.17782137\n",
      "0.180586\n",
      "0.21862385\n",
      "0.19495721\n",
      "0.16256702\n",
      "0.14264432\n",
      "0.1490917\n",
      "0.14314935\n",
      "0.16070972\n",
      "0.18914773\n",
      "0.20030588\n",
      "0.23814707\n",
      "0.15208648\n",
      "0.2246742\n",
      "0.14215069\n",
      "0.2226019\n",
      "0.10566166\n",
      "0.1633998\n",
      "0.18606551\n",
      "0.16562873\n",
      "0.15741692\n",
      "0.106976226\n",
      "0.17917897\n",
      "0.1520758\n",
      "0.19320227\n",
      "0.16255662\n",
      "0.15158272\n",
      "0.15207098\n",
      "0.17487955\n",
      "0.112572886\n",
      "0.15093908\n",
      "0.16930732\n",
      "0.11503744\n",
      "0.17119206\n",
      "0.08880255\n",
      "0.14663981\n",
      "0.1619024\n",
      "0.12575796\n",
      "0.058691777\n",
      "0.17612532\n",
      "0.17931697\n",
      "0.2184675\n",
      "0.1935015\n",
      "0.16212055\n",
      "0.14225778\n",
      "0.14547847\n",
      "0.14081928\n",
      "0.15983385\n",
      "0.18638347\n",
      "0.19255728\n",
      "0.23157808\n",
      "0.14951012\n",
      "0.22110291\n",
      "0.14220253\n",
      "0.2212341\n",
      "0.10694218\n",
      "0.16214028\n",
      "0.1839998\n",
      "0.16289291\n",
      "0.15681681\n",
      "0.10722968\n",
      "0.17785545\n",
      "0.15107252\n",
      "0.19082187\n",
      "0.1607412\n",
      "0.15003026\n",
      "0.15064104\n",
      "0.17367616\n",
      "0.110677354\n",
      "0.14835708\n",
      "0.16990632\n",
      "0.11441082\n",
      "0.16935024\n",
      "0.08555359\n",
      "0.14392112\n",
      "0.16136153\n",
      "0.124048576\n",
      "0.056626905\n",
      "0.1740536\n",
      "0.17792462\n",
      "0.21736118\n",
      "0.19150703\n",
      "0.16194169\n",
      "0.14102042\n",
      "0.14172272\n",
      "0.13859774\n",
      "0.15942584\n",
      "0.18420382\n",
      "0.18634653\n",
      "0.22701477\n",
      "0.14713168\n",
      "0.21875966\n",
      "0.14163582\n",
      "0.22137953\n",
      "0.107189\n",
      "0.16072452\n",
      "0.18241104\n",
      "0.16014104\n",
      "0.15624005\n",
      "0.10630511\n",
      "0.17632043\n",
      "0.14988966\n",
      "0.18821119\n",
      "0.15842387\n",
      "0.14815696\n",
      "0.14932398\n",
      "0.17192423\n",
      "0.10866637\n",
      "0.14620358\n",
      "0.17005888\n",
      "0.11432684\n",
      "0.16732764\n",
      "0.08268327\n",
      "0.14142528\n",
      "0.15967284\n",
      "0.12206091\n",
      "0.05541101\n",
      "0.17162335\n",
      "0.1764996\n",
      "0.21541339\n",
      "0.18840234\n",
      "0.16195884\n",
      "0.13817821\n",
      "0.1372654\n",
      "0.1362046\n",
      "0.1593379\n",
      "0.18173908\n",
      "0.180498\n",
      "0.22247976\n",
      "0.14448984\n",
      "0.21623755\n",
      "0.1406083\n",
      "0.2217172\n",
      "0.10729951\n",
      "0.15876198\n",
      "0.18080509\n",
      "0.15696502\n",
      "0.15570381\n",
      "0.10458081\n",
      "0.17392164\n",
      "0.14831749\n",
      "0.18448822\n",
      "0.15538156\n",
      "0.14573051\n",
      "0.1481195\n",
      "0.16943252\n",
      "0.10633381\n",
      "0.14446622\n",
      "0.16949208\n",
      "0.11465368\n",
      "0.16513422\n",
      "0.07971603\n",
      "0.13915053\n",
      "0.15666293\n",
      "0.11937273\n",
      "0.054509137\n",
      "0.16902485\n",
      "0.175354\n",
      "0.21285586\n",
      "0.18435979\n",
      "0.16243102\n",
      "0.13356721\n",
      "0.13207306\n",
      "0.13384262\n",
      "0.15982054\n",
      "0.17944783\n",
      "0.17548794\n",
      "0.2181621\n",
      "0.14171492\n",
      "0.21399912\n",
      "0.13929781\n",
      "0.22211055\n",
      "0.10765383\n",
      "0.15635765\n",
      "0.17943871\n",
      "0.15353876\n",
      "0.15546617\n",
      "0.10245679\n",
      "0.1707022\n",
      "0.14646773\n",
      "0.17979948\n",
      "0.15214817\n",
      "0.14315364\n",
      "0.14750332\n",
      "0.16661888\n",
      "0.10391987\n",
      "0.14345172\n",
      "0.16845663\n",
      "0.11522463\n",
      "0.16337949\n",
      "0.07654231\n",
      "0.13753168\n",
      "0.15337096\n",
      "0.11632765\n",
      "0.053147703\n",
      "0.16715318\n",
      "0.17521624\n",
      "0.21086164\n",
      "0.18057567\n",
      "0.1635084\n",
      "0.12831548\n",
      "0.12695064\n",
      "0.13209212\n",
      "0.16081826\n",
      "0.17779669\n",
      "0.17172173\n",
      "0.21403114\n",
      "0.13953091\n",
      "0.21215022\n",
      "0.13852002\n",
      "0.22201739\n",
      "0.109024584\n",
      "0.15424843\n",
      "0.17867199\n",
      "0.1508153\n",
      "0.15568297\n",
      "0.100716114\n",
      "0.16752629\n",
      "0.14477822\n",
      "0.17543387\n",
      "0.14973642\n",
      "0.14114207\n",
      "0.14770232\n",
      "0.1646342\n",
      "0.10206882\n",
      "0.14326824\n",
      "0.1671469\n",
      "0.11593093\n",
      "0.16191104\n",
      "0.07474657\n",
      "0.13645363\n",
      "0.15044162\n",
      "0.1139533\n",
      "0.05306916\n",
      "0.16520089\n",
      "0.17458352\n",
      "0.20847932\n",
      "0.17774591\n",
      "0.1641587\n",
      "0.12486338\n",
      "0.12354399\n",
      "0.13121086\n",
      "0.1613995\n",
      "0.17780438\n",
      "0.1702267\n",
      "0.21200942\n",
      "0.13788055\n",
      "0.21216556\n",
      "0.13726182\n",
      "0.22247231\n",
      "0.10911648\n",
      "0.15231873\n",
      "0.17827359\n",
      "0.14891447\n",
      "0.15568423\n",
      "0.09929414\n",
      "0.16502371\n",
      "0.14345588\n",
      "0.17205364\n",
      "0.14837252\n",
      "0.14009158\n",
      "0.14837147\n",
      "0.16330437\n",
      "0.10108456\n",
      "0.14318828\n",
      "0.16611657\n",
      "0.11645281\n",
      "0.16088867\n",
      "0.0733137\n",
      "0.13568082\n",
      "0.14858632\n",
      "0.1121814\n",
      "0.052512437\n",
      "0.16385725\n",
      "0.1745484\n",
      "0.20724052\n",
      "0.17580253\n",
      "0.16467547\n",
      "0.12282181\n",
      "0.12107044\n",
      "0.13057488\n",
      "0.16154277\n",
      "0.17772353\n",
      "0.1689536\n",
      "0.2097716\n",
      "0.13684645\n",
      "0.2116259\n",
      "0.13653055\n",
      "0.22210358\n",
      "0.10960524\n",
      "0.15094265\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "batch_size = 256\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(len(train_y) / batch_size)\n",
    "    print(total_batch)\n",
    "    for epoch in range(epoch):\n",
    "        for i in range(total_batch):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            end = end if end<len(train_y) else len(train_y)\n",
    "            x_batch = train_feature_index[start:end]\n",
    "            v_batch = train_feature_value[start:end]\n",
    "            y_batch = train_y[start:end]\n",
    "            feed_dict = {\n",
    "                feat_index: x_batch,\n",
    "                feat_value: v_batch,\n",
    "                label: y_batch\n",
    "            }\n",
    "            l,opt = sess.run([loss,optimizer],feed_dict=feed_dict)\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
