{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\",\n",
    "    \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\", \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\",\n",
    "    \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\", \"ps_calc_18_bin\",\n",
    "    \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "dfTrain = pd.read_csv(train_file)\n",
    "dfTest = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([dfTrain, dfTest])\n",
    "# 特征字典，key是每一列，即每个field，value是每个值对应的feature_id\n",
    "feature_dict = {}\n",
    "# 特征总数量\n",
    "total_feature = 0\n",
    "for col in df.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        # 数字类型列，作为一个特征\n",
    "        feature_dict[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        # 查看这一列有多少个unique的值\n",
    "        unique_val = df[col].unique()\n",
    "        feature_dict[col] = dict(\n",
    "            zip(unique_val,\n",
    "                range(total_feature,\n",
    "                      len(unique_val) + total_feature)))\n",
    "        total_feature += len(unique_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = dfTrain[['target']].values.tolist()\n",
    "dfTrain.drop(['target', 'id'], axis=1, inplace=True)\n",
    "train_feature_index = dfTrain.copy()\n",
    "train_feature_value = dfTrain.copy()\n",
    "for col in train_feature_index.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        train_feature_index.drop(col, axis=1, inplace=True)\n",
    "        train_feature_value.drop(col, axis=1, inplace=True)\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        train_feature_index[col] = feature_dict[col]\n",
    "    else:\n",
    "        train_feature_index[col] = train_feature_index[col].map(\n",
    "            feature_dict[col])\n",
    "        train_feature_value[col] = 1\n",
    "train_y = np.reshape(np.array(train_y), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfm_params = {\n",
    "    \"embedding_size\":8,\n",
    "    \"deep_layers\":[32,32],\n",
    "    \"dropout_deep\":[0.5,0.5,0.5],\n",
    "    \"deep_layer_activation\":tf.nn.relu,\n",
    "    \"epoch\":30,\n",
    "    \"batch_size\":1024,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"optimizer\":\"adam\",\n",
    "    \"batch_norm\":1,\n",
    "    \"batch_norm_decay\":0.995,\n",
    "    \"verbose\":True,\n",
    "    \"random_seed\":0,\n",
    "    \"deep_init_size\":50,\n",
    "    \"use_inner\":False\n",
    "}\n",
    "nfm_params['feature_size'] = total_feature\n",
    "nfm_params['field_size'] = len(train_feature_index.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight初始化\n",
    "weights = dict()\n",
    "weights['feature_embeddings'] = tf.Variable(tf.random_normal(\n",
    "    [nfm_params['feature_size'], nfm_params['embedding_size']],\n",
    "    mean=0.0,\n",
    "    stddev=0.01),\n",
    "                                            name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(\n",
    "    [nfm_params['feature_size'],1], mean=0.0, stddev=0.01),\n",
    "                                      name='feature_bias')\n",
    "num_layers = len(nfm_params['deep_layers'])\n",
    "input_size = nfm_params['embedding_size']\n",
    "glorot = np.sqrt(2.0 / (input_size + nfm_params['deep_layers'][0]))\n",
    "weights['layer_0'] = tf.Variable(\n",
    "    tf.random_normal([input_size, nfm_params['deep_layers'][0]],\n",
    "                     mean=0.0,\n",
    "                     stddev=glorot))\n",
    "weights['bias_0'] = tf.Variable(\n",
    "    tf.random_normal([1, nfm_params['deep_layers'][1]],\n",
    "                     mean=0.0,\n",
    "                     stddev=glorot))\n",
    "\n",
    "for i in range(1, num_layers):\n",
    "    glorot = np.sqrt(\n",
    "        2.0 /\n",
    "        (nfm_params['deep_layers'][i] + nfm_params['deep_layers'][i - 1]))\n",
    "    weights['layer_%d' % i] = tf.Variable(\n",
    "        tf.random_normal(\n",
    "            [nfm_params['deep_layers'][i - 1], nfm_params['deep_layers'][i]],\n",
    "            mean=0.0,\n",
    "            stddev=glorot))\n",
    "    weights['bias_%d' % i] = tf.Variable(\n",
    "        tf.random_normal([1,nfm_params['deep_layers'][i]],\n",
    "                         mean=0.0,\n",
    "                         stddev=glorot))\n",
    "weights['bias'] = tf.Variable(tf.constant(0.1),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_index = tf.placeholder(tf.int32, [None, nfm_params['field_size']],\n",
    "                            name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32, [None, nfm_params['field_size']],\n",
    "                            name='feat_value')\n",
    "label = tf.placeholder(tf.float32, [None, 1], name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8)\n",
      "WARNING:tensorflow:From <ipython-input-8-b2be6bd08212>:35: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "embedding = tf.nn.embedding_lookup(weights['feature_embeddings'], feat_index)\n",
    "# print(embedding.shape)\n",
    "reshape_feat_value = tf.reshape(feat_value, [-1, nfm_params['field_size'], 1])\n",
    "# print(reshape_feat_value.shape)\n",
    "embedding = tf.multiply(embedding, reshape_feat_value)\n",
    "# print(embedding.shape)\n",
    "\n",
    "# first order part\n",
    "first_order_weight = tf.nn.embedding_lookup(weights['feature_bias'],\n",
    "                                            feat_index)\n",
    "first_order = tf.reduce_sum(tf.multiply(first_order_weight, reshape_feat_value), 2)\n",
    "\n",
    "# second order part\n",
    "\n",
    "# sum square part\n",
    "summed_feature_emb = tf.reduce_sum(embedding, 1)\n",
    "summed_feature_emb_square = tf.square(summed_feature_emb)\n",
    "\n",
    "# square sum part\n",
    "squared_feature_emb = tf.square(embedding)\n",
    "squared_sum_feature_emb = tf.reduce_sum(squared_feature_emb, 1)\n",
    "\n",
    "second_order = 0.5 * (summed_feature_emb_square - squared_sum_feature_emb)\n",
    "\n",
    "print(second_order.shape)\n",
    "\n",
    "y_deep = second_order\n",
    "for i in range(0, len(nfm_params['deep_layers'])):\n",
    "    y_deep = tf.matmul(y_deep,\n",
    "                       weights['layer_%d' % i]) + weights['bias_%d' % i]\n",
    "    y_deep = nfm_params['deep_layer_activation'](y_deep)\n",
    "\n",
    "y_bias = weights['bias'] * tf.ones_like(label)\n",
    "out = tf.add_n([\n",
    "    tf.reduce_sum(first_order, axis=1, keep_dims=True),\n",
    "    tf.reduce_sum(y_deep, axis=1, keep_dims=True),y_bias\n",
    "])\n",
    "out = tf.nn.sigmoid(out)\n",
    "loss = tf.losses.log_loss(label, out)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=nfm_params['learning_rate'],\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999,\n",
    "                                   epsilon=1e-8).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "2.1598244\n",
      "2.095179\n",
      "2.0136921\n",
      "1.9714515\n",
      "1.8545756\n",
      "1.7952203\n",
      "1.7039896\n",
      "1.6477356\n",
      "1.5841694\n",
      "1.5193391\n",
      "1.4449953\n",
      "1.4093797\n",
      "1.3379505\n",
      "1.2755582\n",
      "1.2379825\n",
      "1.1718273\n",
      "1.1430907\n",
      "1.0807393\n",
      "1.0330644\n",
      "0.9965128\n",
      "0.96844083\n",
      "0.91296536\n",
      "0.8752416\n",
      "0.83867633\n",
      "0.80963504\n",
      "0.7716607\n",
      "0.73857886\n",
      "0.7134645\n",
      "0.68185925\n",
      "0.6549593\n",
      "0.6293579\n",
      "0.6044177\n",
      "0.5862564\n",
      "0.55785304\n",
      "0.54223627\n",
      "0.51145077\n",
      "0.5021272\n",
      "0.46016574\n",
      "0.45547277\n",
      "0.44494024\n",
      "0.42508113\n",
      "0.4149819\n",
      "0.39013907\n",
      "0.39625907\n",
      "0.37646362\n",
      "0.3790006\n",
      "0.35941312\n",
      "0.34570262\n",
      "0.33719325\n",
      "0.33458787\n",
      "0.30655578\n",
      "0.31536537\n",
      "0.31664115\n",
      "0.28745782\n",
      "0.30694366\n",
      "0.26704672\n",
      "0.2847782\n",
      "0.27921197\n",
      "0.2572776\n",
      "0.22128169\n",
      "0.26993245\n",
      "0.26642174\n",
      "0.28316623\n",
      "0.26976103\n",
      "0.25102434\n",
      "0.23401722\n",
      "0.24198402\n",
      "0.23181556\n",
      "0.24242792\n",
      "0.25380224\n",
      "0.26005858\n",
      "0.27287364\n",
      "0.23202132\n",
      "0.26803297\n",
      "0.22539409\n",
      "0.26003364\n",
      "0.19706944\n",
      "0.23121753\n",
      "0.2432051\n",
      "0.2286931\n",
      "0.22845411\n",
      "0.19338751\n",
      "0.23377329\n",
      "0.21684903\n",
      "0.2390626\n",
      "0.22152212\n",
      "0.21359475\n",
      "0.21345107\n",
      "0.22411892\n",
      "0.18848993\n",
      "0.21141952\n",
      "0.22233391\n",
      "0.18475977\n",
      "0.22216569\n",
      "0.17047188\n",
      "0.20631313\n",
      "0.20890436\n",
      "0.1871458\n",
      "0.1481502\n",
      "0.2191931\n",
      "0.2173096\n",
      "0.24178317\n",
      "0.22633037\n",
      "0.20607576\n",
      "0.1879748\n",
      "0.19904935\n",
      "0.18854961\n",
      "0.20314918\n",
      "0.21843582\n",
      "0.22762081\n",
      "0.24411261\n",
      "0.19502617\n",
      "0.24020159\n",
      "0.18920186\n",
      "0.23244382\n",
      "0.15776458\n",
      "0.1991944\n",
      "0.21447667\n",
      "0.19827351\n",
      "0.19817257\n",
      "0.15669574\n",
      "0.20618697\n",
      "0.18716288\n",
      "0.21405663\n",
      "0.19413579\n",
      "0.18559274\n",
      "0.18589702\n",
      "0.19946876\n",
      "0.15809645\n",
      "0.18529984\n",
      "0.1986984\n",
      "0.15560627\n",
      "0.19965537\n",
      "0.14003521\n",
      "0.18228047\n",
      "0.18644997\n",
      "0.1617145\n",
      "0.117222324\n",
      "0.19930951\n",
      "0.1976719\n",
      "0.22608577\n",
      "0.20861672\n",
      "0.18650015\n",
      "0.16661635\n",
      "0.1788904\n",
      "0.16773865\n",
      "0.18481703\n",
      "0.2025235\n",
      "0.21336912\n",
      "0.23243594\n",
      "0.17699152\n",
      "0.22863549\n",
      "0.17115706\n",
      "0.22066966\n",
      "0.13696454\n",
      "0.18352161\n",
      "0.20106782\n",
      "0.18337317\n",
      "0.1831738\n",
      "0.13668437\n",
      "0.19284964\n",
      "0.17205729\n",
      "0.20228122\n",
      "0.18043044\n",
      "0.17122509\n",
      "0.17172939\n",
      "0.18737486\n",
      "0.141477\n",
      "0.1717398\n",
      "0.18700281\n",
      "0.13954557\n",
      "0.18848391\n",
      "0.1227048\n",
      "0.16963959\n",
      "0.17499319\n",
      "0.14784177\n",
      "0.09908148\n",
      "0.18938288\n",
      "0.1878723\n",
      "0.2191892\n",
      "0.20008062\n",
      "0.17645167\n",
      "0.15513255\n",
      "0.16814469\n",
      "0.1564495\n",
      "0.1753658\n",
      "0.19478926\n",
      "0.20665629\n",
      "0.2277149\n",
      "0.16737351\n",
      "0.22387627\n",
      "0.16140348\n",
      "0.2157129\n",
      "0.12498385\n",
      "0.17530894\n",
      "0.19454351\n",
      "0.1756196\n",
      "0.17528851\n",
      "0.124926835\n",
      "0.18612163\n",
      "0.1639006\n",
      "0.19658467\n",
      "0.17320411\n",
      "0.16342273\n",
      "0.16403592\n",
      "0.18122946\n",
      "0.13179132\n",
      "0.16429715\n",
      "0.18103778\n",
      "0.13016632\n",
      "0.18272932\n",
      "0.112200156\n",
      "0.16261192\n",
      "0.16891241\n",
      "0.13984808\n",
      "0.08779054\n",
      "0.18424194\n",
      "0.18283881\n",
      "0.21632457\n",
      "0.19590524\n",
      "0.17109968\n",
      "0.14865965\n",
      "0.16210844\n",
      "0.15000224\n",
      "0.17031708\n",
      "0.19097772\n",
      "0.20342426\n",
      "0.22607622\n",
      "0.1619863\n",
      "0.22217946\n",
      "0.15588057\n",
      "0.21390904\n",
      "0.11771944\n",
      "0.1708243\n",
      "0.19134246\n",
      "0.17142323\n",
      "0.17097157\n",
      "0.117646486\n",
      "0.1826545\n",
      "0.15929572\n",
      "0.1938157\n",
      "0.16923748\n",
      "0.15898523\n",
      "0.15966783\n",
      "0.17804213\n",
      "0.12584391\n",
      "0.16000019\n",
      "0.17795074\n",
      "0.12442453\n",
      "0.1796646\n",
      "0.105491064\n",
      "0.15848492\n",
      "0.16557893\n",
      "0.13501666\n",
      "0.08042063\n",
      "0.18147387\n",
      "0.18019117\n",
      "0.2153065\n",
      "0.19384547\n",
      "0.16815269\n",
      "0.14485557\n",
      "0.15852687\n",
      "0.14613403\n",
      "0.16753285\n",
      "0.18908578\n",
      "0.20179263\n",
      "0.22575341\n",
      "0.15881053\n",
      "0.22182445\n",
      "0.1526097\n",
      "0.21355341\n",
      "0.113118574\n",
      "0.16826496\n",
      "0.18977107\n",
      "0.16905253\n",
      "0.16850302\n",
      "0.112928316\n",
      "0.18083516\n",
      "0.15657657\n",
      "0.19247077\n",
      "0.16696286\n",
      "0.15633862\n",
      "0.15707305\n",
      "0.17636225\n",
      "0.12201596\n",
      "0.15737984\n",
      "0.17634815\n",
      "0.12076672\n",
      "0.1779624\n",
      "0.10100844\n",
      "0.1559103\n",
      "0.16369466\n",
      "0.13196753\n",
      "0.07542597\n",
      "0.17990032\n",
      "0.17876019\n",
      "0.21509963\n",
      "0.19281243\n",
      "0.1664724\n",
      "0.14253354\n",
      "0.1562671\n",
      "0.14369546\n",
      "0.16594626\n",
      "0.18813469\n",
      "0.20086332\n",
      "0.22592346\n",
      "0.15682779\n",
      "0.2220018\n",
      "0.15058197\n",
      "0.21383062\n",
      "0.11009895\n",
      "0.16672376\n",
      "0.18899563\n",
      "0.16763802\n",
      "0.16701376\n",
      "0.109749764\n",
      "0.17985688\n",
      "0.15488987\n",
      "0.19180876\n",
      "0.16558547\n",
      "0.15467462\n",
      "0.15545301\n",
      "0.17545938\n",
      "0.11944526\n",
      "0.1556784\n",
      "0.17552254\n",
      "0.118358895\n",
      "0.17695302\n",
      "0.09789539\n",
      "0.15418895\n",
      "0.16259223\n",
      "0.1299651\n",
      "0.07194345\n",
      "0.17892572\n",
      "0.17795107\n",
      "0.215204\n",
      "0.19226241\n",
      "0.16547096\n",
      "0.1410642\n",
      "0.15473615\n",
      "0.14207597\n",
      "0.16500421\n",
      "0.18763195\n",
      "0.2001999\n",
      "0.22620218\n",
      "0.1555033\n",
      "0.22232816\n",
      "0.14926063\n",
      "0.21434477\n",
      "0.10805958\n",
      "0.16572729\n",
      "0.18859479\n",
      "0.16672756\n",
      "0.16604875\n",
      "0.10753625\n",
      "0.1793036\n",
      "0.15378106\n",
      "0.19145587\n",
      "0.16468826\n",
      "0.15356195\n",
      "0.15438084\n",
      "0.17495464\n",
      "0.117649905\n",
      "0.15449044\n",
      "0.1751036\n",
      "0.116731085\n",
      "0.17628938\n",
      "0.09565918\n",
      "0.15294527\n",
      "0.16191678\n",
      "0.12859935\n",
      "0.06946393\n",
      "0.17824206\n",
      "0.17745344\n",
      "0.21537924\n",
      "0.19191979\n",
      "0.1648354\n",
      "0.14010008\n",
      "0.15361258\n",
      "0.14093846\n",
      "0.16441123\n",
      "0.18732624\n",
      "0.19959179\n",
      "0.22641695\n",
      "0.15454747\n",
      "0.22262709\n",
      "0.14834976\n",
      "0.21490596\n",
      "0.10665078\n",
      "0.16502205\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "batch_size = 256\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(len(train_y) / batch_size)\n",
    "    print(total_batch)\n",
    "    for epoch in range(epoch):\n",
    "        for i in range(total_batch):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            end = end if end<len(train_y) else len(train_y)\n",
    "            x_batch = train_feature_index[start:end]\n",
    "            v_batch = train_feature_value[start:end]\n",
    "            y_batch = train_y[start:end]\n",
    "            feed_dict = {\n",
    "                feat_index: x_batch,\n",
    "                feat_value: v_batch,\n",
    "                label: y_batch\n",
    "            }\n",
    "            l,opt = sess.run([loss,optimizer],feed_dict=feed_dict)\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
