{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\",\n",
    "    \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\", \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\",\n",
    "    \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\", \"ps_calc_18_bin\",\n",
    "    \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "dfTrain = pd.read_csv(train_file)\n",
    "dfTest = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ps_car_01_cat': {10: 0, 11: 1, 7: 2, 6: 3, 9: 4, 5: 5, 4: 6, 8: 7, 3: 8, 0: 9, 2: 10, 1: 11, -1: 12}, 'ps_car_02_cat': {1: 13, 0: 14}, 'ps_car_03_cat': {-1: 15, 0: 16, 1: 17}, 'ps_car_04_cat': {0: 18, 1: 19, 8: 20, 9: 21, 2: 22, 6: 23, 3: 24, 7: 25, 4: 26, 5: 27}, 'ps_car_05_cat': {1: 28, -1: 29, 0: 30}, 'ps_car_06_cat': {4: 31, 11: 32, 14: 33, 13: 34, 6: 35, 15: 36, 3: 37, 0: 38, 1: 39, 10: 40, 12: 41, 9: 42, 17: 43, 7: 44, 8: 45, 5: 46, 2: 47, 16: 48}, 'ps_car_07_cat': {1: 49, -1: 50, 0: 51}, 'ps_car_08_cat': {0: 52, 1: 53}, 'ps_car_09_cat': {0: 54, 2: 55, 3: 56, 1: 57, -1: 58, 4: 59}, 'ps_car_10_cat': {1: 60, 0: 61, 2: 62}, 'ps_car_11': {2: 63, 3: 64, 1: 65, 0: 66}, 'ps_car_11_cat': {12: 67, 19: 68, 60: 69, 104: 70, 82: 71, 99: 72, 30: 73, 68: 74, 20: 75, 36: 76, 101: 77, 103: 78, 41: 79, 59: 80, 43: 81, 64: 82, 29: 83, 95: 84, 24: 85, 5: 86, 28: 87, 87: 88, 66: 89, 10: 90, 26: 91, 54: 92, 32: 93, 38: 94, 83: 95, 89: 96, 49: 97, 93: 98, 1: 99, 22: 100, 85: 101, 78: 102, 31: 103, 34: 104, 7: 105, 8: 106, 3: 107, 46: 108, 27: 109, 25: 110, 61: 111, 16: 112, 69: 113, 40: 114, 76: 115, 39: 116, 88: 117, 42: 118, 75: 119, 91: 120, 23: 121, 2: 122, 71: 123, 90: 124, 80: 125, 44: 126, 92: 127, 72: 128, 96: 129, 86: 130, 62: 131, 33: 132, 67: 133, 73: 134, 77: 135, 18: 136, 21: 137, 74: 138, 37: 139, 48: 140, 70: 141, 13: 142, 15: 143, 102: 144, 53: 145, 65: 146, 100: 147, 51: 148, 79: 149, 52: 150, 63: 151, 94: 152, 6: 153, 57: 154, 35: 155, 98: 156, 56: 157, 97: 158, 55: 159, 84: 160, 50: 161, 4: 162, 58: 163, 9: 164, 17: 165, 11: 166, 45: 167, 14: 168, 81: 169, 47: 170}, 'ps_car_12': 171, 'ps_car_13': 172, 'ps_car_14': 173, 'ps_car_15': 174, 'ps_ind_01': {2: 175, 1: 176, 5: 177, 0: 178, 4: 179, 3: 180, 6: 181, 7: 182}, 'ps_ind_02_cat': {2: 183, 1: 184, 4: 185, 3: 186, -1: 187}, 'ps_ind_03': {5: 188, 7: 189, 9: 190, 2: 191, 0: 192, 4: 193, 3: 194, 1: 195, 11: 196, 6: 197, 8: 198, 10: 199}, 'ps_ind_04_cat': {1: 200, 0: 201, -1: 202}, 'ps_ind_05_cat': {0: 203, 1: 204, 4: 205, 3: 206, 6: 207, 5: 208, -1: 209, 2: 210}, 'ps_ind_06_bin': {0: 211, 1: 212}, 'ps_ind_07_bin': {1: 213, 0: 214}, 'ps_ind_08_bin': {0: 215, 1: 216}, 'ps_ind_09_bin': {0: 217, 1: 218}, 'ps_ind_10_bin': {0: 219, 1: 220}, 'ps_ind_11_bin': {0: 221, 1: 222}, 'ps_ind_12_bin': {0: 223, 1: 224}, 'ps_ind_13_bin': {0: 225, 1: 226}, 'ps_ind_14': {0: 227, 1: 228, 2: 229, 3: 230}, 'ps_ind_15': {11: 231, 3: 232, 12: 233, 8: 234, 9: 235, 6: 236, 13: 237, 4: 238, 10: 239, 5: 240, 7: 241, 2: 242, 0: 243, 1: 244}, 'ps_ind_16_bin': {0: 245, 1: 246}, 'ps_ind_17_bin': {1: 247, 0: 248}, 'ps_ind_18_bin': {0: 249, 1: 250}, 'ps_reg_01': 251, 'ps_reg_02': 252, 'ps_reg_03': 253}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([dfTrain, dfTest])\n",
    "# 特征字典，key是每一列，即每个field，value是每个值对应的feature_id\n",
    "feature_dict = {}\n",
    "# 特征总数量\n",
    "total_feature = 0\n",
    "for col in df.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        # 数字类型列，作为一个特征\n",
    "        feature_dict[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        # 查看这一列有多少个unique的值\n",
    "        unique_val = df[col].unique()\n",
    "        feature_dict[col] = dict(\n",
    "            zip(unique_val,\n",
    "                range(total_feature,\n",
    "                      len(unique_val) + total_feature)))\n",
    "        total_feature += len(unique_val)\n",
    "print(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = dfTrain[['target']].values.tolist()\n",
    "dfTrain.drop(['target', 'id'], axis=1, inplace=True)\n",
    "train_feature_index = dfTrain.copy()\n",
    "train_feature_value = dfTrain.copy()\n",
    "for col in train_feature_index.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        train_feature_index.drop(col, axis=1, inplace=True)\n",
    "        train_feature_value.drop(col, axis=1, inplace=True)\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        train_feature_index[col] = feature_dict[col]\n",
    "    else:\n",
    "        train_feature_index[col] = train_feature_index[col].map(\n",
    "            feature_dict[col])\n",
    "        train_feature_value[col] = 1\n",
    "train_y = np.reshape(np.array(train_y), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "\"\"\"模型参数\"\"\"\n",
    "xdfm_params = {\n",
    "    \"embedding_size\": 8,\n",
    "    \"dropout_fm\": [1.0, 1.0],\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"cross_layers\" : [80, 80, 40],\n",
    "    \"deep_layer_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"eval_metric\": 'gini_norm',\n",
    "    \"random_seed\": 3\n",
    "}\n",
    "xdfm_params['feature_size'] = total_feature\n",
    "xdfm_params['field_size'] = len(train_feature_index.columns)\n",
    "print(total_feature)\n",
    "print(len(train_feature_index.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight\n",
    "weights = dict()\n",
    "# linear params\n",
    "weights['feature_embeddings'] = tf.Variable(tf.random_normal(\n",
    "    [xdfm_params['feature_size'], xdfm_params['embedding_size']], 0.0, 0.01),\n",
    "                                            name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(\n",
    "    [xdfm_params['feature_size'], 1], 0.0, 0.01),\n",
    "                                      name='feature_bias')\n",
    "linear_size = xdfm_params['field_size'] \n",
    "weights['linear_weight'] = tf.Variable(tf.random_normal(shape=[linear_size, 1]), dtype=tf.float32)\n",
    "weights['linear_bias'] = tf.Variable(tf.random_normal(shape=[1]),dtype=tf.float32)\n",
    "\n",
    "# deep params\n",
    "input_size = xdfm_params['field_size'] * xdfm_params['embedding_size']\n",
    "glort = np.sqrt(2.0 / (input_size + xdfm_params['deep_layers'][0]))\n",
    "weights['deeplayer_weight_0'] = tf.Variable(tf.random_normal(\n",
    "    shape=[input_size, xdfm_params['deep_layers'][0]], mean=0, stddev=glort),\n",
    "                                            dtype=tf.float32)\n",
    "weights['deeplayer_bias_0'] = tf.Variable(\n",
    "    tf.random_normal(shape=[1, xdfm_params['deep_layers'][0]],\n",
    "                     mean=0,\n",
    "                     stddev=glort))\n",
    "\n",
    "for i in range(1, len(xdfm_params['deep_layers'])):\n",
    "    glort = np.sqrt(\n",
    "        2.0 /\n",
    "        (xdfm_params['deep_layers'][i - 1] + xdfm_params['deep_layers'][i]))\n",
    "    weights['deeplayer_weight_%d' % i] = tf.Variable(tf.random_normal(\n",
    "        shape=[\n",
    "            xdfm_params['deep_layers'][i - 1], xdfm_params['deep_layers'][i]\n",
    "        ],\n",
    "        mean=0.0,\n",
    "        stddev=glort),\n",
    "                                                     dtype=tf.float32)\n",
    "    weights['deeplayer_bias_%d' % i] = tf.Variable(tf.random_normal(\n",
    "        shape=[1, xdfm_params['deep_layers'][i]], mean=0.0, stddev=glort),\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "deep_size = xdfm_params['deep_layers'][-1]\n",
    "weights['deep_weight'] = tf.Variable(tf.random_normal(shape=[deep_size, 1]), dtype=tf.float32)\n",
    "weights['deep_bias'] = tf.Variable(tf.random_normal(shape=[1]),dtype=tf.float32)\n",
    "    \n",
    "# cin params\n",
    "weights['cross_layer_0'] = tf.Variable(\n",
    "        tf.random_normal(shape=[\n",
    "            1, xdfm_params['field_size'] * xdfm_params['field_size'], xdfm_params['cross_layers'][0]\n",
    "        ], mean=0.0, stddev=0.1))\n",
    "for i in range(1, len(xdfm_params['cross_layers'])):\n",
    "    weights['cross_layer_%d' % i] = tf.Variable(\n",
    "        tf.random_normal(shape=[\n",
    "            1, xdfm_params['field_size'] *\n",
    "            xdfm_params['cross_layers'][i-1], xdfm_params['cross_layers'][i]\n",
    "        ], mean=0.0, stddev=0.1))\n",
    "\n",
    "cross_size = sum(xdfm_params['cross_layers'])\n",
    "weights['cross_weight'] = tf.Variable(tf.random_normal(shape=[cross_size, 1]), dtype=tf.float32)\n",
    "weights['cross_bias'] = tf.Variable(tf.random_normal(shape=[1]),dtype=tf.float32)\n",
    "\n",
    "\n",
    "# final params \n",
    "weights['final_weight'] = tf.Variable(tf.random_normal(shape=[3, 1]), dtype=tf.float32)\n",
    "weights['final_bias'] = tf.Variable(tf.random_normal(shape=[1]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "feat_index = tf.placeholder(tf.int32, shape=[None, None], name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32, shape=[None, None], name='feat_value')\n",
    "label = tf.placeholder(tf.float32, shape=[None, 1], name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "reshaped_feat_value = tf.reshape(feat_value,\n",
    "                                 shape=[-1, xdfm_params['field_size'], 1])\n",
    "embeddings = tf.nn.embedding_lookup(weights['feature_embeddings'], feat_index)\n",
    "embeddings = tf.multiply(embeddings, reshaped_feat_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# linear part\n",
    "weight = tf.nn.embedding_lookup(weights['feature_bias'], feat_index)\n",
    "linear_out = tf.reduce_sum(tf.multiply(weight, reshaped_feat_value), 2)\n",
    "linear_out = tf.matmul(linear_out , weights['linear_weight']) + weights['linear_bias']\n",
    "print(linear_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 37, 8)\n",
      "(?, 37, 1)\n",
      "(8, ?, 37, 37)\n",
      "(8, ?, 1369)\n",
      "(?, 8, 1369)\n",
      "(1, 1369, 80)\n",
      "(?, 8, 80)\n",
      "(?, 80, 8)\n",
      "(?, 80, 8)\n",
      "(?, 80, 1)\n",
      "(8, ?, 37, 80)\n",
      "(8, ?, 2960)\n",
      "(?, 8, 2960)\n",
      "(1, 2960, 80)\n",
      "(?, 8, 80)\n",
      "(?, 80, 8)\n",
      "(?, 80, 8)\n",
      "(?, 80, 1)\n",
      "(8, ?, 37, 80)\n",
      "(8, ?, 2960)\n",
      "(?, 8, 2960)\n",
      "(1, 2960, 40)\n",
      "(?, 8, 40)\n",
      "(?, 40, 8)\n",
      "(?, 200, 8)\n"
     ]
    }
   ],
   "source": [
    "# cin part\n",
    "cross_layers = []\n",
    "field_nums = []\n",
    "final_result = []\n",
    "# 对输入进行处理\n",
    "reshape_cin_input = tf.reshape(\n",
    "    embeddings,\n",
    "    shape=[-1, xdfm_params['field_size'], xdfm_params['embedding_size']])\n",
    "cross_layers.append(reshape_cin_input)\n",
    "field_nums.append(int(xdfm_params['field_size']))\n",
    "final_len = 0\n",
    "split_tensor_0 = tf.split(cross_layers[0], xdfm_params['embedding_size']*[1], 2)\n",
    "for idx, layer_size in enumerate(xdfm_params['cross_layers']):\n",
    "    print(cross_layers[-1].shape)\n",
    "    split_tensor = tf.split(cross_layers[-1], xdfm_params['embedding_size']*[1], 2)\n",
    "    print(split_tensor[0].shape)\n",
    "    # 计算外积\n",
    "    dot_result_m = tf.matmul(split_tensor_0, split_tensor, transpose_b=True)\n",
    "    print(dot_result_m.shape)\n",
    "    dot_result_o = tf.reshape(dot_result_m,\n",
    "                              shape=[\n",
    "                                  xdfm_params['embedding_size'], -1,\n",
    "                                  field_nums[0] * field_nums[-1]\n",
    "                              ])\n",
    "    print(dot_result_o.shape)\n",
    "    dot_result = tf.transpose(dot_result_o, perm=[1, 0, 2])    \n",
    "    print(dot_result.shape)\n",
    "    filters = weights['cross_layer_%d' % idx]\n",
    "    print(filters.shape)\n",
    "    curr_out = tf.nn.conv1d(dot_result,\n",
    "                            filters=filters,\n",
    "                            stride=1,\n",
    "                            padding=\"VALID\")\n",
    "    print(curr_out.shape)\n",
    "    curr_out = tf.nn.relu(curr_out)\n",
    "    curr_out = tf.transpose(curr_out, perm=[0, 2, 1])\n",
    "    print(curr_out.shape)\n",
    "    direct_connect = curr_out\n",
    "    next_hidden = curr_out\n",
    "    final_len += layer_size\n",
    "    field_nums.append(int(layer_size))\n",
    "    final_result.append(direct_connect)\n",
    "    cross_layers.append(next_hidden)\n",
    "result = tf.concat(final_result, axis=1)\n",
    "print(result.shape)\n",
    "cin_out = tf.reduce_sum(result, -1)\n",
    "cin_out = tf.matmul(cin_out, weights['cross_weight']) + weights['cross_bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# dnn part\n",
    "deep_out = tf.reshape(embeddings, [-1, xdfm_params['field_size'] * xdfm_params['embedding_size']])\n",
    "for i in range(len(xdfm_params['deep_layers'])):\n",
    "    deep_out = tf.matmul(deep_out, weights['deeplayer_weight_%d'%i]) + weights['deeplayer_bias_%d'%i]\n",
    "    deep_out = tf.nn.relu(deep_out)\n",
    "deep_out = tf.matmul(deep_out, weights['deep_weight']) + weights['deep_bias']\n",
    "print(deep_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = tf.concat([linear_out, cin_out, deep_out], axis=1)\n",
    "res = tf.matmul(final_layer,weights['final_weight'] ) + weights['final_bias']\n",
    "logit = tf.nn.sigmoid(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.log_loss(label, logit)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=xdfm_params['learning_rate']).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4881177\n",
      "2.476877\n",
      "2.4656577\n",
      "2.4544654\n",
      "2.4433217\n",
      "2.4322414\n",
      "2.4212332\n",
      "2.410305\n",
      "2.3994555\n",
      "2.3755052\n",
      "2.3648849\n",
      "2.3543231\n",
      "2.3438103\n",
      "2.3333373\n",
      "2.3228934\n",
      "2.3124666\n",
      "2.3020468\n",
      "2.2916288\n",
      "2.298889\n",
      "2.2883945\n",
      "2.2778764\n",
      "2.2673316\n",
      "2.2567606\n",
      "2.2461588\n",
      "2.2355254\n",
      "2.2248566\n",
      "2.2141492\n",
      "2.1999784\n",
      "2.1892438\n",
      "2.1784656\n",
      "2.1676397\n",
      "2.156764\n",
      "2.145834\n",
      "2.1348476\n",
      "2.123803\n",
      "2.1126952\n",
      "2.1131983\n",
      "2.101869\n",
      "2.0904686\n",
      "2.079003\n",
      "2.0674734\n",
      "2.0558946\n",
      "2.044274\n",
      "2.0326266\n",
      "2.020966\n",
      "1.9938593\n",
      "1.9823277\n",
      "1.9707787\n",
      "1.9592164\n",
      "1.9476277\n",
      "1.9359907\n",
      "1.9242959\n",
      "1.9125276\n",
      "1.9006746\n",
      "1.887008\n",
      "1.8749936\n",
      "1.8628726\n",
      "1.850639\n",
      "1.8382951\n",
      "1.8258338\n",
      "1.8132558\n",
      "1.8005549\n",
      "1.7877265\n",
      "1.7691326\n",
      "1.756096\n",
      "1.7429289\n",
      "1.7296286\n",
      "1.7161901\n",
      "1.702614\n",
      "1.688897\n",
      "1.6750376\n",
      "1.661035\n",
      "1.642071\n",
      "1.6279202\n",
      "1.6136248\n",
      "1.5991838\n",
      "1.5845969\n",
      "1.5698615\n",
      "1.5549754\n",
      "1.5399368\n",
      "1.5247443\n",
      "1.5214747\n",
      "1.5057855\n",
      "1.48992\n",
      "1.4738764\n",
      "1.4576578\n",
      "1.4412652\n",
      "1.4247016\n",
      "1.4079703\n",
      "1.391076\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size = int(len(train_feature_index)/xdfm_params['batch_size'])\n",
    "    for i in range(xdfm_params['epoch']):\n",
    "        for j in range(batch_size):\n",
    "            start = i * xdfm_params['batch_size']\n",
    "            end = (i+1) * xdfm_params['batch_size']\n",
    "            end = end if end<len(train_feature_index) else len(train_feature_index)\n",
    "            feat_index_batch = train_feature_index[start:end]\n",
    "            feat_value_batch = train_feature_value[start:end]\n",
    "            label_batch = train_y[start:end]\n",
    "            feed_dict = {\n",
    "                feat_index:feat_index_batch,\n",
    "                feat_value:feat_value_batch,\n",
    "                label:label_batch\n",
    "            }\n",
    "            l,o = sess.run([loss,optimizer], feed_dict)\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
