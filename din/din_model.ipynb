{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data00/home/huangyajian/miniconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)  # (user_id, history, pos, label)\n",
    "    test_set = pickle.load(f)\n",
    "    cate_list = pickle.load(f)\n",
    "    user_count, item_count, cate_count = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(103944, [17704, 37473], 53346, 0), (126219, [15082, 19768, 30450], 48620, 0), (21022, [11725, 4058, 6186, 16970, 13703, 17403, 12736, 30082, 11580, 14475, 32516, 4463, 14413, 25965, 1387, 24154, 48052, 54593, 5021, 32991, 49609, 2111, 10099, 26271, 56329, 15856, 41125, 54526, 51815, 28690], 2179, 0), (134145, [4952, 33036], 55798, 1), (121815, [9906, 30126, 25972, 38025], 45235, 1)]\n",
      "[(114981, [24265, 8283, 43319, 45475], (56743, 31087)), (50580, [42354, 19191, 13067, 33134, 33800, 58952], (43604, 12255)), (172652, [28839, 32507, 34101, 34339], (49205, 2002)), (98577, [299, 5127, 6140, 14805, 15916, 20120], (17961, 29586)), (147658, [4932, 3674, 10339, 19888], (20257, 45347))]\n",
      "[738 157 571 ...  63 674 351]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0:5])\n",
    "print(test_set[0:5])\n",
    "print(cate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_id\n",
    "user = tf.compat.v1.placeholder(tf.int32, [\n",
    "    None,\n",
    "], name='user')\n",
    "# 正样本的item\n",
    "pos = tf.compat.v1.placeholder(tf.int32, [\n",
    "    None,\n",
    "], name='pos')\n",
    "# 负样本的item\n",
    "neg = tf.compat.v1.placeholder(tf.int32, [\n",
    "    None,\n",
    "], name='neg')\n",
    "# label\n",
    "y = tf.compat.v1.placeholder(tf.float32, [\n",
    "    None,\n",
    "], name='y')\n",
    "# 用户行为特征(User Behavior)中的item序列\n",
    "hist_i = tf.compat.v1.placeholder(tf.int32, [None, None], name='hist_i')\n",
    "# User Behavior中序列的真实序列长度\n",
    "length = tf.compat.v1.placeholder(tf.int32, [\n",
    "    None,\n",
    "], name='length')\n",
    "hidden_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63001, 64)\n",
      "(801, 64)\n"
     ]
    }
   ],
   "source": [
    "# user_emb_w = tf.get_variable('user_emb_w',[user_count, hidden_units])\n",
    "# print(user_emb_w.shape)\n",
    "# item本身的embedding\n",
    "item_emb_w = tf.get_variable('item_emb_w', [item_count, hidden_units // 2])\n",
    "print(item_emb_w.shape)\n",
    "item_b = tf.get_variable('item_b', [item_count],\n",
    "                         initializer=tf.constant_initializer(0.0))\n",
    "# item所属类别的embedding\n",
    "cate_emb_w = tf.get_variable('cate_emb_w', [cate_count, hidden_units // 2])\n",
    "print(cate_emb_w.shape)\n",
    "cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(queries, keys, keys_length):\n",
    "    \"\"\"\n",
    "        queries:     [B, H]    [batch_size,embedding_size]\n",
    "        keys:        [B, T, H]   [batch_size,T,embedding_size]\n",
    "        keys_length: [B]        [batch_size]\n",
    "        #T为历史行为序列长度\n",
    "    \"\"\"\n",
    "    queries_hidden_units = queries.get_shape().as_list()[-1]  #B*H->H\n",
    "    queries = tf.tile(queries, [1, tf.shape(keys)[1]])  # B*T\n",
    "    queries = tf.reshape(\n",
    "        queries, [-1, tf.shape(keys)[1], queries_hidden_units])  # B*T*H\n",
    "    din_all = tf.concat([queries, keys, queries - keys, queries * keys],\n",
    "                        axis=-1)  # B*T*4H\n",
    "    # 三层全链接(d_layer_3_all为训练出来的atteneion权重）\n",
    "    d_layer_1_all = tf.layers.dense(din_all,\n",
    "                                    80,\n",
    "                                    activation=tf.nn.sigmoid,\n",
    "                                    name='f1_att')\n",
    "    d_layer_2_all = tf.layers.dense(d_layer_1_all,\n",
    "                                    40,\n",
    "                                    activation=tf.nn.sigmoid,\n",
    "                                    name='f2_att')\n",
    "    d_layer_3_all = tf.layers.dense(d_layer_2_all,\n",
    "                                    1,\n",
    "                                    activation=None,\n",
    "                                    name='f3_att') # B*T*1\n",
    "    #为了让outputs维度和keys的维度一致\n",
    "    outputs = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(keys)[1]])  # B*1*T\n",
    "    #  bool类型 tf.shape(keys)[1]为历史行为序列的最大长度，keys_length为人为设定的参数，\n",
    "    #  如tf.sequence_mask(5，3)  即为array[True,True,True,False,False]\n",
    "    #  函数的作用是为了后面补齐行为序列，获取等长的行为序列做铺垫\n",
    "    key_masks = tf.sequence_mask(keys_length, tf.shape(keys)[1])  # B*T\n",
    "    #在第二维增加一维，也就是由B*T变成B*1*T\n",
    "    key_masks = tf.expand_dims(key_masks, 1)  # B*1*T\n",
    "    #tf.ones_like新建一个与output类型大小一致的tensor，设置填充值为一个很小的值，而不是0,padding的mask后补一个很小的负数，这样softmax之后就会接近0\n",
    "    paddings = tf.ones_like(outputs) * (-2**32 + 1)  # B*1*T\n",
    "    # key_masks是bool型值，True/False，返回值是对应元素，key_masks中元素为True的元素替换为outputs中的元素，为False的元素替换为paddings中对应元素\n",
    "    outputs = tf.where(key_masks, outputs, paddings)\n",
    "    # Scale（缩放）\n",
    "    outputs = outputs / (keys.get_shape().as_list()[-1]**0.5)\n",
    "    # Activation\n",
    "    outputs = tf.nn.softmax(outputs)  # B * 1 * T\n",
    "    #Weighted Sum outputs=g(Vi,Va)   keys=Vi\n",
    "    # 这步为公式中的g(Vi*Va)*Vi\n",
    "    outputs = tf.matmul(\n",
    "        outputs,\n",
    "        keys)  # B * 1 * H 三维矩阵相乘，相乘发生在后两维，即 B * (( 1 * T ) * ( T * H ))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(_x, axis=-1, epsilon=0.0000001, name=''):\n",
    "    alphas = tf.get_variable('alpha' + name,\n",
    "                             _x.get_shape()[-1],\n",
    "                             initializer=tf.constant_initializer(0.0),\n",
    "                             dtype=tf.float32)\n",
    "    input_shape = list(_x.get_shape())\n",
    "    reduction_axes = list(range(len(input_shape)))\n",
    "    del reduction_axes[axis]\n",
    "\n",
    "    broadcast_shape = [1] * len(input_shape)\n",
    "    broadcast_shape[axis] = input_shape[axis]\n",
    "\n",
    "    mean = tf.reduce_mean(_x, axis=reduction_axes)\n",
    "    broadcast_mean = tf.reshape(mean, broadcast_shape)\n",
    "    std = tf.reduce_mean(tf.square(_x - broadcast_mean) + epsilon,\n",
    "                         axis=reduction_axes)\n",
    "    std = tf.sqrt(std)\n",
    "    broadcast_std = tf.reshape(std, broadcast_shape)\n",
    "    # x_normed = (_x - brodcast_mean) / (brodcast_std + epsilon)\n",
    "    # 以上是手动求均值方差，下面是直接调用bn接口\n",
    "    x_normed = tf.layers.batch_normalization(_x, center=False, scale=False)\n",
    "    x_p = tf.sigmoid(x_normed)\n",
    "    return alphas * (1 - x_p) * _x + x_p * _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-9305030e9b9b>:18: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /data00/home/huangyajian/miniconda3/lib/python3.7/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "(?, ?, 1)\n",
      "WARNING:tensorflow:From <ipython-input-8-023f3f535758>:34: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "# 从cate_list中取出正样本的cate\n",
    "pos_cate = tf.gather(cate_list, pos)\n",
    "# 正样本的embedding，正样本包括item和cate\n",
    "pos_emb = tf.concat(\n",
    "    [\n",
    "        tf.nn.embedding_lookup(item_emb_w, pos),  # (?, 64)\n",
    "        tf.nn.embedding_lookup(cate_emb_w, pos_cate)  # (?, 64)\n",
    "    ],\n",
    "    axis=1)\n",
    "# 偏置b\n",
    "pos_b = tf.gather(item_b, pos)\n",
    "# 从cate_list中取出负样本的cate\n",
    "neg_cate = tf.gather(cate_list, neg)\n",
    "# 负样本的embedding，负样本包括item和cate\n",
    "neg_emb = tf.concat(\n",
    "    [\n",
    "        tf.nn.embedding_lookup(item_emb_w, neg),  # (?,64)\n",
    "        tf.nn.embedding_lookup(cate_emb_w, neg_cate)  # (?,64)\n",
    "    ],\n",
    "    axis=1)\n",
    "# 偏置b\n",
    "neg_b = tf.gather(item_b, neg)\n",
    "# 用户行为序列(User Behavior)中的cate序列\n",
    "hist_cate = tf.gather(cate_list, hist_i)\n",
    "# 用户行为序列(User Behavior)的embedding，包括item序列和cate序列\n",
    "hist_emb = tf.concat(\n",
    "    [\n",
    "        tf.nn.embedding_lookup(item_emb_w, hist_i),  # (?,?,64)\n",
    "        tf.nn.embedding_lookup(cate_emb_w, hist_cate)  # (?,?,64)\n",
    "    ],\n",
    "    axis=2)\n",
    "# attention操作\n",
    "hist = attention(pos_emb, hist_emb, length)\n",
    "hist = tf.layers.batch_normalization(inputs=hist)\n",
    "hist = tf.reshape(hist, [-1, hidden_units])\n",
    "hist = tf.layers.dense(hist, hidden_units)\n",
    "u_emb = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面两个全连接用来计算y'，i为正样本，j为负样本\n",
    "din_pos = tf.concat([u_emb, pos_emb], axis=-1)\n",
    "din_pos = tf.layers.batch_normalization(inputs=din_pos, name='b1')\n",
    "d_layer_1_pos = tf.layers.dense(din_pos, 80, activation=None, name='f1')\n",
    "d_layer_1_pos = dice(d_layer_1_pos, name='dice_1_pos')\n",
    "d_layer_2_pos = tf.layers.dense(d_layer_1_pos, 40, activation=None, name='f2')\n",
    "d_layer_2_pos = dice(d_layer_2_pos, name='dice_2_pos')\n",
    "d_layer_3_pos = tf.layers.dense(d_layer_2_pos, 1, activation=None, name='f3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "din_neg = tf.concat([u_emb, neg_emb], axis=-1)\n",
    "din_neg = tf.layers.batch_normalization(inputs=din_neg, name='b1', reuse=True)\n",
    "d_layer_1_neg = tf.layers.dense(din_neg,\n",
    "                                80,\n",
    "                                activation=None,\n",
    "                                name='f1',\n",
    "                                reuse=True)\n",
    "d_layer_1_neg = dice(d_layer_1_neg, name='dice_1_neg')\n",
    "d_layer_2_neg = tf.layers.dense(d_layer_1_neg,\n",
    "                                40,\n",
    "                                activation=None,\n",
    "                                name='f2',\n",
    "                                reuse=True)\n",
    "d_layer_2_neg = dice(d_layer_2_neg, name='dice_2_neg')\n",
    "d_layer_3_neg = tf.layers.dense(d_layer_2_neg,\n",
    "                                1,\n",
    "                                activation=None,\n",
    "                                name='f3',\n",
    "                                reuse=True)\n",
    "\n",
    "d_layer_3_pos = tf.reshape(d_layer_3_pos, [-1])\n",
    "d_layer_3_neg = tf.reshape(d_layer_3_neg, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测的（y正-y负）\n",
    "x = pos_b - neg_b + d_layer_3_pos - d_layer_3_neg\n",
    "#预测的（y正）\n",
    "logits = pos_b + d_layer_3_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_emb_all = tf.expand_dims(u_emb, 1)\n",
    "# u_emb_all = tf.tile(u_emb_all, [1, item_count, 1])\n",
    "# all_emb = tf.concat([item_emb_w, tf.nn.embedding_lookup(cate_emb_w, cate_list)], axis=1)\n",
    "# all_emb = tf.expand_dims(all_emb, 0)\n",
    "# all_emb = tf.tile(all_emb, [512, 1, 1])\n",
    "# din_all = tf.concat([u_emb_all, all_emb], axis=-1)\n",
    "# din_all = tf.layers.batch_normalization(inputs=din_all, name='b1', reuse=True)\n",
    "# d_layer_1_all = tf.layers.dense(din_all, 80, activation=None, name='f1', reuse=True)\n",
    "# d_layer_1_all = dice(d_layer_1_all, name='dice_1_all')\n",
    "# d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=None, name='f2', reuse=True)\n",
    "# d_layer_2_all = dice(d_layer_2_all, name='dice_2_all')\n",
    "# d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3', reuse=True)\n",
    "# d_layer_3_all = tf.reshape(d_layer_3_all, [-1, item_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits_all = tf.sigmoid(item_b+d_layer_3_all)\n",
    "mf_auc = tf.reduce_mean(tf.to_float(x > 0))\n",
    "score_pos = tf.sigmoid(pos_b + d_layer_3_pos)\n",
    "score_neg = tf.sigmoid(neg_b + d_layer_3_neg)\n",
    "score_pos = tf.reshape(score_pos, [-1, 1])\n",
    "score_neg = tf.reshape(score_neg, [-1, 1])\n",
    "p_and_n = tf.concat([score_pos, score_neg], axis=-1)\n",
    "# tf.summary.scalar(\"auc\", mf_auc)\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "tf.summary.scalar(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(i, batch_size):\n",
    "    start = j * batch_size\n",
    "    end = (j + 1) * batch_size\n",
    "    end = end if end < len(train_set) else len(train_set)\n",
    "    train_batch = train_set[start:end]\n",
    "    user_batch = [i[0] for i in train_batch]\n",
    "    pos_batch = [i[2] for i in train_batch]  # 本次点击的样本id\n",
    "    y_batch = [i[3] for i in train_batch\n",
    "               ]  # 本次点击id是否存在于用户历史点击序列中，负样本是自己构造的，肯定不在用户点击历史中\n",
    "    length_batch = [len(i[1]) for i in train_batch]\n",
    "    hist_i_batch = np.zeros(\n",
    "        [len(train_batch), max(length_batch)], np.int64)  # 用户点击历史\n",
    "    k = 0\n",
    "    for t in train_batch:\n",
    "        for l in range(len(t[1])):\n",
    "            hist_i_batch[k][l] = t[1][l]\n",
    "        k += 1\n",
    "    return (user_batch, pos_batch, y_batch, hist_i_batch, length_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch_data(i, batch_size):\n",
    "    start = j * batch_size\n",
    "    end = (j + 1) * batch_size\n",
    "    end = end if end < len(test_set) else len(test_set)\n",
    "    test_batch = test_set[start:end]\n",
    "    user_batch = [i[0] for i in test_batch]\n",
    "    pos_batch = [i[2][0] for i in test_batch]  # 正样本\n",
    "    neg_batch = [i[2][1] for i in test_batch]  # 负样本\n",
    "    length_batch = [len(i[1]) for i in test_batch]\n",
    "    hist_i_batch = np.zeros(\n",
    "        [len(test_batch), max(length_batch)], np.int64)  # 用户点击历史\n",
    "    k = 0\n",
    "    for t in test_batch:\n",
    "        for l in range(len(t[1])):\n",
    "            hist_i_batch[k][l] = t[1][l]\n",
    "        k += 1\n",
    "    return (user_batch, pos_batch, neg_batch, hist_i_batch, length_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6932454\n",
      "loss:  0.6935494\n",
      "loss:  0.6903191\n",
      "loss:  0.69957167\n",
      "loss:  0.68729526\n",
      "loss:  0.7010373\n",
      "loss:  0.69755274\n",
      "loss:  0.69126606\n",
      "loss:  0.6945267\n",
      "loss:  0.69850993\n",
      "loss:  0.6907612\n",
      "loss:  0.6931665\n",
      "loss:  0.6852367\n",
      "loss:  0.6977018\n",
      "loss:  0.6953786\n",
      "loss:  0.70441073\n",
      "loss:  0.68979686\n",
      "loss:  0.69872284\n",
      "loss:  0.69000983\n",
      "loss:  0.68639374\n",
      "loss:  0.6987846\n",
      "loss:  0.6899035\n",
      "loss:  0.6900839\n",
      "loss:  0.6907774\n",
      "loss:  0.7016809\n",
      "loss:  0.69172144\n",
      "loss:  0.6969315\n",
      "loss:  0.6931871\n",
      "loss:  0.6912251\n",
      "loss:  0.6898962\n",
      "loss:  0.69575423\n",
      "loss:  0.6948594\n",
      "loss:  0.6893753\n",
      "loss:  0.703921\n",
      "loss:  0.69313335\n",
      "loss:  0.6937824\n",
      "loss:  0.69240403\n",
      "loss:  0.6931088\n",
      "loss:  0.6939084\n",
      "loss:  0.69281924\n",
      "loss:  0.6936897\n",
      "loss:  0.69431525\n",
      "loss:  0.69107765\n",
      "loss:  0.7019503\n",
      "loss:  0.6933042\n",
      "loss:  0.69249773\n",
      "loss:  0.6921172\n",
      "loss:  0.692208\n",
      "loss:  0.69221187\n",
      "loss:  0.6960041\n",
      "loss:  0.69264174\n",
      "loss:  0.6931572\n",
      "loss:  0.6903663\n",
      "loss:  0.6955194\n",
      "loss:  0.6919825\n",
      "loss:  0.69635665\n",
      "loss:  0.6920087\n",
      "loss:  0.6916161\n",
      "loss:  0.6929209\n",
      "loss:  0.68889374\n",
      "loss:  0.69415706\n",
      "loss:  0.69409835\n",
      "loss:  0.6913104\n",
      "loss:  0.6918461\n",
      "loss:  0.69421273\n",
      "loss:  0.69194126\n",
      "loss:  0.6916368\n",
      "loss:  0.6879027\n",
      "loss:  0.70264167\n",
      "loss:  0.6911599\n",
      "loss:  0.69118\n",
      "loss:  0.6934885\n",
      "loss:  0.694046\n",
      "loss:  0.6929662\n",
      "loss:  0.6924194\n",
      "loss:  0.6922904\n",
      "loss:  0.6914077\n",
      "loss:  0.6862351\n",
      "loss:  0.6965426\n",
      "loss:  0.70134294\n",
      "loss:  0.69262964\n",
      "loss:  0.6900134\n",
      "loss:  0.6854732\n",
      "loss:  0.702886\n",
      "loss:  0.6876726\n",
      "loss:  0.69636315\n",
      "loss:  0.6893711\n",
      "loss:  0.69473183\n",
      "loss:  0.69395673\n",
      "loss:  0.6940569\n",
      "loss:  0.69097245\n",
      "loss:  0.6932171\n",
      "loss:  0.69094425\n",
      "loss:  0.6936097\n",
      "loss:  0.69596195\n",
      "loss:  0.69188017\n",
      "loss:  0.6915678\n",
      "loss:  0.69341743\n",
      "loss:  0.69285756\n",
      "loss:  0.6955383\n",
      "loss:  0.6945992\n",
      "loss:  0.69062126\n",
      "loss:  0.694884\n",
      "loss:  0.69486946\n",
      "loss:  0.69225574\n",
      "loss:  0.6946909\n",
      "loss:  0.69123334\n",
      "loss:  0.69076526\n",
      "loss:  0.6961324\n",
      "loss:  0.6945034\n",
      "loss:  0.69260156\n",
      "loss:  0.6918382\n",
      "loss:  0.6970869\n",
      "loss:  0.69196486\n",
      "loss:  0.6929661\n",
      "loss:  0.6942189\n",
      "loss:  0.69205534\n",
      "loss:  0.6924067\n",
      "loss:  0.69476086\n",
      "loss:  0.6919329\n",
      "loss:  0.693876\n",
      "loss:  0.6926633\n",
      "loss:  0.6957105\n",
      "loss:  0.6968194\n",
      "loss:  0.6922978\n",
      "loss:  0.6933917\n",
      "loss:  0.6887625\n",
      "loss:  0.69893885\n",
      "loss:  0.6957016\n",
      "loss:  0.69218487\n",
      "loss:  0.6969379\n",
      "loss:  0.69276905\n",
      "loss:  0.6916944\n",
      "loss:  0.69117606\n",
      "loss:  0.69580925\n",
      "loss:  0.6921866\n",
      "loss:  0.69183457\n",
      "loss:  0.69271964\n",
      "loss:  0.6916815\n",
      "loss:  0.69050294\n",
      "loss:  0.687859\n",
      "loss:  0.6793661\n",
      "loss:  0.7102519\n",
      "loss:  0.6920093\n",
      "loss:  0.6916194\n",
      "loss:  0.69511294\n",
      "loss:  0.6933022\n",
      "loss:  0.69290197\n",
      "loss:  0.69249827\n",
      "loss:  0.69285244\n",
      "loss:  0.693797\n",
      "loss:  0.69256854\n",
      "loss:  0.6932943\n",
      "loss:  0.69151014\n",
      "loss:  0.69370955\n",
      "loss:  0.69193447\n",
      "loss:  0.69181985\n",
      "loss:  0.69424003\n",
      "loss:  0.69305646\n",
      "loss:  0.6926074\n",
      "loss:  0.6944021\n",
      "loss:  0.69166386\n",
      "loss:  0.69246554\n",
      "loss:  0.69190896\n",
      "loss:  0.6918546\n",
      "loss:  0.69079494\n",
      "loss:  0.6896144\n",
      "loss:  0.6897775\n",
      "loss:  0.6976492\n",
      "loss:  0.69394016\n",
      "loss:  0.6929009\n",
      "loss:  0.6911249\n",
      "loss:  0.6875271\n",
      "loss:  0.69947153\n",
      "loss:  0.6929097\n",
      "loss:  0.69063354\n",
      "loss:  0.69418573\n",
      "loss:  0.69029224\n",
      "loss:  0.6906178\n",
      "loss:  0.6915163\n",
      "loss:  0.6954547\n",
      "loss:  0.6927127\n",
      "loss:  0.6918876\n",
      "loss:  0.6911453\n",
      "loss:  0.694701\n",
      "loss:  0.6926036\n",
      "loss:  0.6969776\n",
      "loss:  0.6916966\n",
      "loss:  0.69092476\n",
      "loss:  0.6927841\n",
      "loss:  0.69187635\n",
      "loss:  0.6929863\n",
      "loss:  0.6896813\n",
      "loss:  0.6911216\n",
      "loss:  0.6964705\n",
      "loss:  0.6911751\n",
      "loss:  0.69243276\n",
      "loss:  0.6930153\n",
      "loss:  0.69268245\n",
      "loss:  0.6922221\n",
      "loss:  0.6905819\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-90fc2f4714ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mhist_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist_i_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 })\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "epoches = 20\n",
    "batch_size = 128\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"/data00/home/huangyajian/code/tf_log/\",\n",
    "                                   sess.graph)\n",
    "    for i in range(epoches):\n",
    "        batch = (int)(len(train_set) / batch_size)\n",
    "        for j in range(batch):\n",
    "            user_batch, pos_batch, y_batch, hist_i_batch, length_batch = get_batch_data(\n",
    "                j, batch_size)\n",
    "            l, _ = sess.run(\n",
    "                [loss, train_op],\n",
    "                feed_dict={\n",
    "                    user: user_batch,\n",
    "                    pos: pos_batch,\n",
    "                    hist_i: hist_i_batch,\n",
    "                    y: y_batch,\n",
    "                    length: length_batch\n",
    "                })\n",
    "            print(\"loss: \", l)\n",
    "            if j % 100 == 0:\n",
    "                #                 user_batch_test, pos_batch_test, neg_batch_test, hist_i_batch_test, length_batch_test = get_test_batch_data(j, batch_size)\n",
    "                #                 rs = sess.run(\n",
    "                #                     [merged],\n",
    "                #                     feed_dict={\n",
    "                #                         user: user_batch_test,\n",
    "                #                         pos: pos_batch_test,\n",
    "                #                         neg: neg_batch_test,\n",
    "                #                         hist_i: hist_i_batch_test,\n",
    "                #                         length: length_batch_test\n",
    "                #                     })\n",
    "                #                 writer.add_summary(rs, j)\n",
    "                rs = sess.run(merged,\n",
    "                              feed_dict={\n",
    "                                  user: user_batch,\n",
    "                                  pos: pos_batch,\n",
    "                                  hist_i: hist_i_batch,\n",
    "                                  y: y_batch,\n",
    "                                  length: length_batch\n",
    "                              })\n",
    "                writer.add_summary(rs, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
