{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "NUMERIC_COLS = [\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\",\n",
    "    \"ps_car_14\", \"ps_car_15\"\n",
    "]\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\", \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\",\n",
    "    \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\", \"ps_calc_18_bin\",\n",
    "    \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]\n",
    "dfTrain = pd.read_csv(train_file)\n",
    "dfTest = pd.read_csv(test_file)\n",
    "# dfTrain.head(10)\n",
    "# dfTrain['ps_reg_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ps_car_01_cat': {10: 0, 11: 1, 7: 2, 6: 3, 9: 4, 5: 5, 4: 6, 8: 7, 3: 8, 0: 9, 2: 10, 1: 11, -1: 12}, 'ps_car_02_cat': {1: 13, 0: 14}, 'ps_car_03_cat': {-1: 15, 0: 16, 1: 17}, 'ps_car_04_cat': {0: 18, 1: 19, 8: 20, 9: 21, 2: 22, 6: 23, 3: 24, 7: 25, 4: 26, 5: 27}, 'ps_car_05_cat': {1: 28, -1: 29, 0: 30}, 'ps_car_06_cat': {4: 31, 11: 32, 14: 33, 13: 34, 6: 35, 15: 36, 3: 37, 0: 38, 1: 39, 10: 40, 12: 41, 9: 42, 17: 43, 7: 44, 8: 45, 5: 46, 2: 47, 16: 48}, 'ps_car_07_cat': {1: 49, -1: 50, 0: 51}, 'ps_car_08_cat': {0: 52, 1: 53}, 'ps_car_09_cat': {0: 54, 2: 55, 3: 56, 1: 57, -1: 58, 4: 59}, 'ps_car_10_cat': {1: 60, 0: 61, 2: 62}, 'ps_car_11': {2: 63, 3: 64, 1: 65, 0: 66}, 'ps_car_11_cat': {12: 67, 19: 68, 60: 69, 104: 70, 82: 71, 99: 72, 30: 73, 68: 74, 20: 75, 36: 76, 101: 77, 103: 78, 41: 79, 59: 80, 43: 81, 64: 82, 29: 83, 95: 84, 24: 85, 5: 86, 28: 87, 87: 88, 66: 89, 10: 90, 26: 91, 54: 92, 32: 93, 38: 94, 83: 95, 89: 96, 49: 97, 93: 98, 1: 99, 22: 100, 85: 101, 78: 102, 31: 103, 34: 104, 7: 105, 8: 106, 3: 107, 46: 108, 27: 109, 25: 110, 61: 111, 16: 112, 69: 113, 40: 114, 76: 115, 39: 116, 88: 117, 42: 118, 75: 119, 91: 120, 23: 121, 2: 122, 71: 123, 90: 124, 80: 125, 44: 126, 92: 127, 72: 128, 96: 129, 86: 130, 62: 131, 33: 132, 67: 133, 73: 134, 77: 135, 18: 136, 21: 137, 74: 138, 37: 139, 48: 140, 70: 141, 13: 142, 15: 143, 102: 144, 53: 145, 65: 146, 100: 147, 51: 148, 79: 149, 52: 150, 63: 151, 94: 152, 6: 153, 57: 154, 35: 155, 98: 156, 56: 157, 97: 158, 55: 159, 84: 160, 50: 161, 4: 162, 58: 163, 9: 164, 17: 165, 11: 166, 45: 167, 14: 168, 81: 169, 47: 170}, 'ps_car_12': 171, 'ps_car_13': 172, 'ps_car_14': 173, 'ps_car_15': 174, 'ps_ind_01': {2: 175, 1: 176, 5: 177, 0: 178, 4: 179, 3: 180, 6: 181, 7: 182}, 'ps_ind_02_cat': {2: 183, 1: 184, 4: 185, 3: 186, -1: 187}, 'ps_ind_03': {5: 188, 7: 189, 9: 190, 2: 191, 0: 192, 4: 193, 3: 194, 1: 195, 11: 196, 6: 197, 8: 198, 10: 199}, 'ps_ind_04_cat': {1: 200, 0: 201, -1: 202}, 'ps_ind_05_cat': {0: 203, 1: 204, 4: 205, 3: 206, 6: 207, 5: 208, -1: 209, 2: 210}, 'ps_ind_06_bin': {0: 211, 1: 212}, 'ps_ind_07_bin': {1: 213, 0: 214}, 'ps_ind_08_bin': {0: 215, 1: 216}, 'ps_ind_09_bin': {0: 217, 1: 218}, 'ps_ind_10_bin': {0: 219, 1: 220}, 'ps_ind_11_bin': {0: 221, 1: 222}, 'ps_ind_12_bin': {0: 223, 1: 224}, 'ps_ind_13_bin': {0: 225, 1: 226}, 'ps_ind_14': {0: 227, 1: 228, 2: 229, 3: 230}, 'ps_ind_15': {11: 231, 3: 232, 12: 233, 8: 234, 9: 235, 6: 236, 13: 237, 4: 238, 10: 239, 5: 240, 7: 241, 2: 242, 0: 243, 1: 244}, 'ps_ind_16_bin': {0: 245, 1: 246}, 'ps_ind_17_bin': {1: 247, 0: 248}, 'ps_ind_18_bin': {0: 249, 1: 250}, 'ps_reg_01': 251, 'ps_reg_02': 252, 'ps_reg_03': 253}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([dfTrain, dfTest])\n",
    "# 特征字典，key是每一列，即每个field，value是每个值对应的feature_id\n",
    "feature_dict = {}\n",
    "# 特征总数量\n",
    "total_feature = 0\n",
    "for col in df.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        # 数字类型列，作为一个特征\n",
    "        feature_dict[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        # 查看这一列有多少个unique的值\n",
    "        unique_val = df[col].unique()\n",
    "        feature_dict[col] = dict(\n",
    "            zip(unique_val,\n",
    "                range(total_feature,\n",
    "                      len(unique_val) + total_feature)))\n",
    "        total_feature += len(unique_val)\n",
    "print(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = dfTrain[['target']].values.tolist()\n",
    "dfTrain.drop(['target', 'id'], axis=1, inplace=True)\n",
    "train_feature_index = dfTrain.copy()\n",
    "train_feature_value = dfTrain.copy()\n",
    "for col in train_feature_index.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        train_feature_index.drop(col, axis=1, inplace=True)\n",
    "        train_feature_value.drop(col, axis=1, inplace=True)\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        train_feature_index[col] = feature_dict[col]\n",
    "    else:\n",
    "        train_feature_index[col] = train_feature_index[col].map(\n",
    "            feature_dict[col])\n",
    "        train_feature_value[col] = 1\n",
    "train_y = np.reshape(np.array(train_y), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "\"\"\"模型参数\"\"\"\n",
    "dfm_params = {\n",
    "    \"use_fm\": True,\n",
    "    \"use_deep\": True,\n",
    "    \"embedding_size\": 8,\n",
    "    \"dropout_fm\": [1.0, 1.0],\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layer_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"eval_metric\": 'gini_norm',\n",
    "    \"random_seed\": 3\n",
    "}\n",
    "dfm_params['feature_size'] = total_feature\n",
    "dfm_params['field_size'] = len(train_feature_index.columns)\n",
    "print(total_feature)\n",
    "print(len(train_feature_index.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "feat_index = tf.placeholder(tf.int32, shape=[None, None], name='feat_index')\n",
    "feat_value = tf.placeholder(tf.float32, shape=[None, None], name='feat_value')\n",
    "label = tf.placeholder(tf.float32, shape=[None, 1], name='label')\n",
    "# weight\n",
    "weights = dict()\n",
    "# liearn weight\n",
    "weights['feature_embeddings'] = tf.Variable(tf.random_normal(\n",
    "    [dfm_params['feature_size'], dfm_params['embedding_size']], 0.0, 0.01),\n",
    "                                            name='feature_embeddings')\n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal(\n",
    "    [dfm_params['feature_size'], 1], 0.0, 0.01),\n",
    "                                      name='feature_bias')\n",
    "# deep weight\n",
    "num_layer = len(dfm_params['deep_layers'])\n",
    "# 这里重点注意，deep层输入个数\n",
    "input_size = dfm_params['field_size'] * dfm_params['embedding_size']\n",
    "glorot = np.sqrt(2.0 / (input_size + dfm_params['deep_layers'][0]))\n",
    "weights['layer_0'] = tf.Variable(np.random.normal(\n",
    "    loc=0, scale=glorot, size=(input_size, dfm_params['deep_layers'][0])),\n",
    "                                 dtype=np.float32)\n",
    "weights['bias_0'] = tf.Variable(np.random.normal(\n",
    "    loc=0, scale=glorot, size=(1, dfm_params['deep_layers'][0])),\n",
    "                                dtype=np.float32)\n",
    "for i in range(1, num_layer):\n",
    "    glorot = np.sqrt(\n",
    "        2.0 /\n",
    "        (dfm_params['deep_layers'][i - 1] + dfm_params['deep_layers'][i]))\n",
    "    weights[\"layer_%d\" % i] = tf.Variable(np.random.normal(\n",
    "        loc=0,\n",
    "        scale=glorot,\n",
    "        size=(dfm_params['deep_layers'][i - 1], dfm_params['deep_layers'][i])),\n",
    "                                          dtype=np.float32)\n",
    "    weights[\"bias_%d\" % i] = tf.Variable(np.random.normal(\n",
    "        loc=0, scale=glorot, size=(1, dfm_params['deep_layers'][i])),\n",
    "                                         dtype=np.float32)\n",
    "input_size = dfm_params['field_size'] + dfm_params[\n",
    "    'embedding_size'] + dfm_params['deep_layers'][-1]\n",
    "glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "weights['concat_projection'] = tf.Variable(np.random.normal(loc=0,\n",
    "                                                            scale=glorot,\n",
    "                                                            size=(input_size,\n",
    "                                                                  1)),\n",
    "                                           dtype=np.float32)\n",
    "weights['concat_bias'] = tf.Variable(tf.constant(0.01), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 37)\n"
     ]
    }
   ],
   "source": [
    "# fm part\n",
    "# 一次项部分\n",
    "# reshaped_feat_value的形状是(?, 37, 1)\n",
    "reshaped_feat_value = tf.reshape(feat_value,\n",
    "                                 shape=[-1, dfm_params['field_size'], 1])\n",
    "# 形状是(?,37,1)，就是w * x\n",
    "fm_first_order = tf.nn.embedding_lookup(weights['feature_bias'], feat_index)\n",
    "fm_first_order = tf.reduce_sum(\n",
    "    tf.multiply(fm_first_order, reshaped_feat_value), 2)\n",
    "print(fm_first_order.shape)\n",
    "\n",
    "# 二次项部分\n",
    "# embeddings的形状是(254，8)，一共254个特征，每个特征embedsize是8\n",
    "embeddings = tf.nn.embedding_lookup(weights['feature_embeddings'], feat_index)\n",
    "# 形状为(?,37,8)，这里计算的是v_i * x_i，fm公式里包含这一项\n",
    "embeddings = tf.multiply(embeddings, reshaped_feat_value)\n",
    "# 形状为(?, 8)\n",
    "summed_features_emb = tf.reduce_sum(embeddings, 1)\n",
    "summed_features_emb_square = tf.square(summed_features_emb)\n",
    "\n",
    "squared_features_emb = tf.square(embeddings)\n",
    "squared_sum_features_emb = tf.reduce_sum(squared_features_emb, 1)\n",
    "# 形状为(?, 8)\n",
    "fm_second_order = 0.5 * tf.subtract(summed_features_emb_square,\n",
    "                                    squared_sum_features_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep part\n",
    "\n",
    "# 形状(?, 296)\n",
    "y_deep = tf.reshape(\n",
    "    embeddings,\n",
    "    shape=[-1, dfm_params['field_size'] * dfm_params['embedding_size']])\n",
    "for i in range(0,len(dfm_params['deep_layers'])):\n",
    "    y_deep = tf.add(tf.matmul(y_deep,weights[\"layer_%d\" %i]), weights[\"bias_%d\"%i])\n",
    "    y_deep = tf.nn.relu(y_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(?, 77)，37 + 8 + 32，其中fm部分输出维度为field_size + embedding_size，deep部分是32\n",
    "concat_input = tf.concat([fm_first_order, fm_second_order, y_deep], axis=1)\n",
    "out = tf.nn.sigmoid(\n",
    "    tf.add(tf.matmul(concat_input, weights['concat_projection']),\n",
    "           weights['concat_bias']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huangyajian/repo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\"\"\"loss and optimizer\"\"\"\n",
    "loss = tf.losses.log_loss(tf.reshape(label, (-1, 1)), out)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=dfm_params['learning_rate'],\n",
    "                                   beta1=0.9,\n",
    "                                   beta2=0.999,\n",
    "                                   epsilon=1e-8).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,loss is 0.7904554\n",
      "epoch 1,loss is 0.7777381\n",
      "epoch 2,loss is 0.76574767\n",
      "epoch 3,loss is 0.7544462\n",
      "epoch 4,loss is 0.7433578\n",
      "epoch 5,loss is 0.732174\n",
      "epoch 6,loss is 0.72087646\n",
      "epoch 7,loss is 0.7096608\n",
      "epoch 8,loss is 0.69849986\n",
      "epoch 9,loss is 0.687696\n",
      "epoch 10,loss is 0.6772066\n",
      "epoch 11,loss is 0.66655195\n",
      "epoch 12,loss is 0.65562814\n",
      "epoch 13,loss is 0.64439714\n",
      "epoch 14,loss is 0.6329011\n",
      "epoch 15,loss is 0.6214559\n",
      "epoch 16,loss is 0.61020094\n",
      "epoch 17,loss is 0.5988858\n",
      "epoch 18,loss is 0.5877118\n",
      "epoch 19,loss is 0.576404\n",
      "epoch 20,loss is 0.5647501\n",
      "epoch 21,loss is 0.5533871\n",
      "epoch 22,loss is 0.5419371\n",
      "epoch 23,loss is 0.5301462\n",
      "epoch 24,loss is 0.51809543\n",
      "epoch 25,loss is 0.50573486\n",
      "epoch 26,loss is 0.49304345\n",
      "epoch 27,loss is 0.4799881\n",
      "epoch 28,loss is 0.4665479\n",
      "epoch 29,loss is 0.45272022\n",
      "epoch 30,loss is 0.43854293\n",
      "epoch 31,loss is 0.42414662\n",
      "epoch 32,loss is 0.40954432\n",
      "epoch 33,loss is 0.3947473\n",
      "epoch 34,loss is 0.37980595\n",
      "epoch 35,loss is 0.36478946\n",
      "epoch 36,loss is 0.3497751\n",
      "epoch 37,loss is 0.3348465\n",
      "epoch 38,loss is 0.32009265\n",
      "epoch 39,loss is 0.30560678\n",
      "epoch 40,loss is 0.29148415\n",
      "epoch 41,loss is 0.27782032\n",
      "epoch 42,loss is 0.2647105\n",
      "epoch 43,loss is 0.25233907\n",
      "epoch 44,loss is 0.24090971\n",
      "epoch 45,loss is 0.23026311\n",
      "epoch 46,loss is 0.2204093\n",
      "epoch 47,loss is 0.21138361\n",
      "epoch 48,loss is 0.20321095\n",
      "epoch 49,loss is 0.19590215\n",
      "epoch 50,loss is 0.18945381\n",
      "epoch 51,loss is 0.18384853\n",
      "epoch 52,loss is 0.17905484\n",
      "epoch 53,loss is 0.17503162\n",
      "epoch 54,loss is 0.17171495\n",
      "epoch 55,loss is 0.16905244\n",
      "epoch 56,loss is 0.166973\n",
      "epoch 57,loss is 0.16540682\n",
      "epoch 58,loss is 0.16428359\n",
      "epoch 59,loss is 0.16353212\n",
      "epoch 60,loss is 0.16308765\n",
      "epoch 61,loss is 0.16288593\n",
      "epoch 62,loss is 0.16286802\n",
      "epoch 63,loss is 0.1629829\n",
      "epoch 64,loss is 0.16318582\n",
      "epoch 65,loss is 0.16343851\n",
      "epoch 66,loss is 0.16370915\n",
      "epoch 67,loss is 0.16397221\n",
      "epoch 68,loss is 0.16420825\n",
      "epoch 69,loss is 0.1644032\n",
      "epoch 70,loss is 0.16454782\n",
      "epoch 71,loss is 0.16463703\n",
      "epoch 72,loss is 0.16466917\n",
      "epoch 73,loss is 0.16464536\n",
      "epoch 74,loss is 0.16456892\n",
      "epoch 75,loss is 0.16444474\n",
      "epoch 76,loss is 0.16427884\n",
      "epoch 77,loss is 0.16407798\n",
      "epoch 78,loss is 0.16384919\n",
      "epoch 79,loss is 0.16359961\n",
      "epoch 80,loss is 0.16333613\n",
      "epoch 81,loss is 0.16306522\n",
      "epoch 82,loss is 0.16279285\n",
      "epoch 83,loss is 0.16252422\n",
      "epoch 84,loss is 0.16226384\n",
      "epoch 85,loss is 0.16201541\n",
      "epoch 86,loss is 0.16178179\n",
      "epoch 87,loss is 0.1615651\n",
      "epoch 88,loss is 0.1613666\n",
      "epoch 89,loss is 0.16118692\n",
      "epoch 90,loss is 0.16102597\n",
      "epoch 91,loss is 0.1608831\n",
      "epoch 92,loss is 0.16075721\n",
      "epoch 93,loss is 0.16064675\n",
      "epoch 94,loss is 0.1605499\n",
      "epoch 95,loss is 0.1604646\n",
      "epoch 96,loss is 0.16038798\n",
      "epoch 97,loss is 0.16031682\n",
      "epoch 98,loss is 0.16024949\n",
      "epoch 99,loss is 0.16018565\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        epoch_loss,_ = sess.run([loss,optimizer],feed_dict={feat_index:train_feature_index,\n",
    "                             feat_value:train_feature_value,\n",
    "                             label:train_y})\n",
    "        print(\"epoch %s,loss is %s\" % (str(i),str(epoch_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
